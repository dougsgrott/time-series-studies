{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5677c9c",
   "metadata": {},
   "source": [
    "\n",
    "## üìò Stationarity in Time Series ‚Äì Focused Intuition\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Section 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7885bd8",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "This post is meant to provide a concise but comprehensive overview of the concept of stationarity and of the different types of stationarity defined in academic literature dealing with time series analysis.\n",
    "\n",
    "Future posts will aim to provide similarly concise overviews of detection of non-stationarity in time series data and of the different ways to transform non-stationary time series into stationary ones.¬π\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Predicting the next data point in a time series is very valuable and is called forecasting.\n",
    "\n",
    "One requirement to accurately forecast the next data point is to ensure that the time series is stationary. In this article, we will discuss:\n",
    "\n",
    "- What a stationary time series is\n",
    "- How to make a time series stationary\n",
    "- How to test that a time series is indeed stationary\n",
    "- Why we need a stationary time series\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "Stationarity describes the concept that the statistical features of a time series do not change over time.\n",
    "\n",
    "Thus, some time series forecasting models, such as autoregressive models, rely on the stationarity of the time series.\n",
    "\n",
    "In this article, you will learn:\n",
    "\n",
    "- What stationarity is,\n",
    "- Why it is important,\n",
    "- Ways to check for stationarity, and\n",
    "- Techniques you can apply when a time series is non-stationary\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1224c",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "## 1Ô∏è‚É£ What is Stationarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458f4f6",
   "metadata": {},
   "source": [
    "## Raw Definition\n",
    "\n",
    "\n",
    "Intuitively, stationarity means that the statistical properties of the process do not change over time. However, several different notions of stationarity have been suggested in econometric literature over the years.\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "**Stationarity** means that the **statistical properties** of the time series (mean, variance, autocorrelation) are **constant over time**.\n",
    "\n",
    "(from alternative prompt)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Stationarity describes the concept that how a time series is changing will remain the same in the future [3].\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Let‚Äôs look at some definition I‚Äôve found a while back:\n",
    "\n",
    "    Stationarity implies that taking consecutive samples of data with the same size should have identical covariances regardless of the starting point.\n",
    "\n",
    "Not that easy to process, I know, but let‚Äôs break it down. The above definition is a definition for something known as weak-form stationarity, or ‚Äúcovariance stationarity‚Äù, as stated in some resources. There exists one other type of stationarity, called strict stationarity, and it implies that samples of identical size have identical distribution. This form is very restrictive, and we rarely observe it, so for doing TSA, the term ‚Äústationarity‚Äù is used to describe covariance stationarity.\n",
    "\n",
    "(from What is Stationarity in Time Series and why should you care)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b25132",
   "metadata": {},
   "source": [
    "## Key Characteristics\n",
    "\n",
    "A time series is **stationary** if its **statistical properties do not change over time**. These properties include:\n",
    "\n",
    "* **Mean** (average value)\n",
    "* **Variance** (spread or variability)\n",
    "* **Covariance structure** (how values relate to each other over time)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "In general, a time series is stationary if it does not exhibit any long term trends or obvious seasonality. Mathematically we have:\n",
    "\n",
    "- A constant variance through time\n",
    "- A constant mean through time\n",
    "- The statistical properties of the time series do not change\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "A time series has to satisfy the following conditions to be considered stationary:\n",
    "\n",
    "- Constant mean ‚Äî average value doesn‚Äôt change over time.\n",
    "- Constant variance ‚Äî variance doesn‚Äôt change over time.\n",
    "- Constant covariance ‚Äî covariance between periods of identical length doesn‚Äôt change over time.\n",
    "\n",
    "(from Introduction to stationarity)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "1. Constant Mean: A stationary time series should exhibit a consistent average value over time. If the mean changes, it suggests a shift in the underlying behavior of the process.\n",
    "2. Constant Variance: The variance of the time series, representing the spread of data points, should remain constant. Fluctuations in variance can make it challenging to make accurate predictions.\n",
    "3. Constant Autocorrelation: Autocorrelation measures the correlation between a time series and its lagged values. In a stationary series, the strength and pattern of autocorrelation should be consistent throughout.\n",
    "\n",
    "(from Understanding Predictive Maintenance ‚Äî Unit Roots and Stationarity)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "In mathematical terms, a time series is stationary when its statistical properties are independent of time [3]:\n",
    "\n",
    "- constant mean,\n",
    "- constant variance,\n",
    "- and covariance is independent of time.\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Okay, I get that, but what does it mean for time series to be stationary? This one‚Äôs easy. To some time series to be classified as stationary (covariance stationarity), it must satisfy 3 conditions:\n",
    "\n",
    "1. Constant mean\n",
    "2. Constant variance\n",
    "3. Constant covariance between periods of identical distance\n",
    "\n",
    "The last one might be a bit trickier to understand at first, so let‚Äôs explore it a bit further. All it states is that the covariance between time periods of identical lengths (let‚Äôs say 10 days/hours/minutes) should be identical to the covariance of some other period of the same length:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from What is Stationarity in Time Series and why should you care)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cf605",
   "metadata": {},
   "source": [
    "## Other\n",
    "\n",
    "\n",
    "An important distinction to make before diving into these definitions is that stationarity ‚Äî of any kind ‚Äî is a property of a stochastic process, and not of any finite or infinite realization of it (i.e. a time series of values).\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a9562",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# =================================================\n",
    "\n",
    "# 2Ô∏è‚É£ Types of Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed353cb",
   "metadata": {},
   "source": [
    "## Weak and Strong\n",
    "\n",
    "\n",
    "There are two main types:\n",
    "\n",
    "* **Strict Stationarity**: Distribution is constant across time (stronger).\n",
    "* **Weak Stationarity**: Mean and variance are constant; covariance depends only on lag, not time.\n",
    "\n",
    "(from alternative prompt)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "This is the definition of weak-form stationarity. Another type of stationarity is strict stationarity. It implies that samples of identical size have identical distribution [5]. Since strict stationarity is restrictive and rare, this article will only focus on weak-form stationarity.\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Let‚Äôs start by examining one of the formal definitions:\n",
    "\n",
    "    A stationary process is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time. (Source: Wikipedia)\n",
    "\n",
    "The above definition tells you what weak-form stationarity is. That‚Äôs the only form you should care about in time series analysis.\n",
    "\n",
    "The other form is strict stationarity. It implies that samples of identical size have identical distribution. It is very restrictive, so you won‚Äôt see it often in practice.\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "### Strong stationarity\n",
    "\n",
    "Strong stationarity requires the shift-invariance (in time) of the finite-dimensional distributions of a stochastic process. This means that the distribution of a finite sub-sequence of random variables of the stochastic process remains the same as we shift it along the time index axis. For example, all i.i.d. stochastic processes are stationary.¬≥\n",
    "\n",
    "Formally, the discrete stochastic process ùëø={x·µ¢ ; i‚àà‚Ñ§} is stationary if\n",
    "\n",
    "EQUATION\n",
    "\n",
    "for T‚äÇ‚Ñ§ with n‚àà‚Ñï and any œÑ‚àà‚Ñ§. [Cox & Miller, 1965] For continuous stochastic processes the condition is similar, with T‚äÇ‚Ñù, n‚àà‚Ñï and any œÑ‚àà‚Ñù instead.\n",
    "\n",
    "This is the most common definition of stationarity, and it is commonly referred to simply as stationarity. It is sometimes also referred to as strict-sense stationarity or strong-sense stationarity.\n",
    "\n",
    "    Note: This definition does not assume the existence/finiteness of any moment of the random variables composing the stochastic process!\n",
    "\n",
    "### Weak stationarity\n",
    "\n",
    "Weak stationarity only requires the shift-invariance (in time) of the first moment and the cross moment (the auto-covariance). This means the process has the same mean at all time points, and that the covariance between the values at any two time points, t and t‚àík, depend only on k, the difference between the two times, and not on the location of the points along the time axis.\n",
    "\n",
    "Formally, the process {x·µ¢ ; i‚àà‚Ñ§} is weakly stationary if:\n",
    "1. The first moment of x·µ¢ is constant; i.e. ‚àÄt, E[x·µ¢]=ùúá\n",
    "2. The second moment of x·µ¢ is finite for all t; i.e. ‚àÄt, E[x·µ¢¬≤]<‚àû (which also implies of course E[(x·µ¢-ùúá)¬≤]<‚àû; i.e. that variance is finite for all t)\n",
    "3. The cross moment ‚Äî i.e. the auto-covariance ‚Äî depends only on the difference u-v; i.e. ‚àÄu,v,a, cov(x·µ§, x·µ•)=cov(x·µ§‚Çä‚Çê, x·µ•‚Çä‚Çê)\n",
    "\n",
    "The third condition implies that every lag ùúè‚àà‚Ñï has a constant covariance value associated with it:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Note that this directly implies that the variance of the process is also constant, since we get that for all t‚àà‚Ñï\n",
    "\n",
    "EQUATION\n",
    "\n",
    "This paints a specific picture of weakly stationary processes as those with constant mean and variance. Their properties are contrasted nicely with those of their counterparts in Figure 2 below.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Other common names for weak stationarity are wide-sense stationarity, weak-sense stationarity, covariance stationarity and second order stationarity¬≤. Confusingly enough, it is also sometimes referred to simply as stationarity, depending on context (see [Boshnakov, 2011] for an example); in geo-statistical literature, for example, this is the dominant notion of stationarity. [Myers, 1989]\n",
    "\n",
    "    Note: Strong stationarity does not imply weak stationarity, nor does the latter implies the former (see example here)! An exception are Gaussian processes, for which weak stationarity does imply strong stationarity.\n",
    "    The reason strong stationarity does not imply weak stationarity is that it does not mean the process necessarily has a finite second moment; e.g. an IID process with standard Cauchy distribution is strictly stationary but has no finite second moment‚Å¥ (see [Myers, 1989]). Indeed, having a finite second moment is a necessary and sufficient condition for the weak stationarity of a strongly stationary process.\n",
    "\n",
    "White Noise Process: A white noise process is a serially uncorrelated stochastic process with a mean of zero and a constant and finite variance.\n",
    "\n",
    "Formally, the process {x·µ¢ ; i‚àà‚Ñ§} is a white noise process if:\n",
    "1. The first moment of x·µ¢ is always zero; i.e. ‚àÄt, E[x·µ¢]=0\n",
    "2. The second moment of x·µ¢ is finite for all t; i.e. ‚àÄt, E[(x·µ¢-ùúá)¬≤]<‚àû\n",
    "3. The cross moment E[x·µ§ x·µ•] is zero when u‚â†v; i.e. ‚àÄu,v w. u‚â†v, cov(x·µ§, x·µ•)=0\n",
    "\n",
    "Note that this implies that every white noise process is a weak stationary process. If, additionally, every variable x·µ¢ follows a normal distribution with zero mean and the same variance œÉ¬≤, then the process is said to be a Gaussian white noise process.\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864d0a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### N-th order stationarity\n",
    "\n",
    "Very close to the definition of strong stationarity, N-th order stationarity demands the shift-invariance (in time) of the distribution of any n samples of the stochastic process, for all n up to order N.\n",
    "\n",
    "Thus, the same condition is required:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "for T‚äÇ‚Ñ§ with n‚àà{1,‚Ä¶,N} and any œÑ‚àà‚Ñ§.\n",
    "\n",
    "Naturally, stationarity to a certain order N does not imply stationarity of any higher order (but the inverse is true). An interesting thread in mathoverflow showcases both an example of a 1st order stationary process that is not 2nd order stationary, and an example for a 2nd order stationary process that is not 3rd order stationary.\n",
    "\n",
    "Note that stationarity of the N-th order for N=2 is surprisingly not equivalent to weak stationarity, even though the latter is sometimes referred to as second-order stationarity. [Myers, 1989] Like with strong stationarity, the condition which 2nd order stationarity sets for the distribution of any two samples of ùëø does not imply that ùëø has finite moments. And similarly, having a finite second moment is a sufficient and necessary condition for a 2nd order stationary process to also be a weakly stationary process.\n",
    "\n",
    "### First-order stationarity\n",
    "\n",
    "The term first-order stationarity is sometimes used to describe a series that has means that never changes with time, but for which any other moment (like variance) can change.[Boshnakov, 2011]\n",
    "\n",
    "Again, note that this definition is not equivalent to N-th order stationarity for N=1, as the latter entails that x·µ¢ are all identically distributed for a process ùëø={x·µ¢ ; i‚àà‚Ñ§}. For example, a process where x·µ¢~ùìù(ùúá,f(i)) where f(i)=1 for even values of i and f(i)=2 for odd values has a constant mean over time, but x·µ¢ are not identically distributed. As a result, such a process pertains to this specific definition of first-order stationarity, but not to N-th order stationarity for N=1.\n",
    "\n",
    "### Cyclostationarity\n",
    "\n",
    "A stochastic process is cyclostationary if the joint distribution of any set of samples is invariant over a time shift of mP, where m‚àà‚Ñ§ and P‚àà‚Ñï is the period of the process:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Cyclostationarity is prominent in signal processing.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "### Trend stationarity\n",
    "\n",
    "A stochastic process is trend stationary if an underlying trend (function solely of time) can be removed, leaving a stationary process. Meaning, the process can be expressed as y·µ¢=f(i)+Œµ·µ¢, where f(i) is any function f:‚Ñù‚Üí‚Ñù and Œµ·µ¢ is a stationary stochastic process with a mean of zero.\n",
    "\n",
    "In the presence of a shock (a significant and rapid one-off change to the value of the series), trend-stationary processes are mean-reverting; i.e. over time, the series will converge again towards the growing (or shrinking) mean, which is not affected by the shock.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "### Joint stationarity\n",
    "\n",
    "Intuitive extensions exist of all of the above types of stationarity for pairs of stochastic processes. For example, for a pair of stochastic process ùëø and ùíÄ, joint strong stationarity is defined by the same condition of strong stationarity, but is simply imposed on the joint cumulative distribution function of the two processes. Weak stationarity and N-th order stationarity can be extended in the same way (the latter to M-N-th order joint stationarity).\n",
    "\n",
    "### The intrinsic hypothesis\n",
    "\n",
    "A weaker form of weak stationarity, prominent in geostatistical literature (see [Myers 1989] and [Fischer et al. 1996], for example). The intrinsic hypothesis holds for a stochastic process ùëø={X·µ¢} if:\n",
    "\n",
    "1. The expected difference between values at any two places separated by distance r is zero: E[x·µ¢-x·µ¢‚Çä·µ£]=0\n",
    "2. The variance of differences, given by Var[x·µ¢-x·µ¢‚Çä·µ£], exists (i.e. it‚Äôs finite) and depends only the distance r.\n",
    "\n",
    "This notion implies weak stationarity of the difference X·µ¢-X·µ¢‚Çä·µ£, and was extended with a definition of N-th order intrinsic hypothesis.\n",
    "\n",
    "### Locally stationary stochastic processes\n",
    "\n",
    "An important class of non-stationary processes are locally stationary (LS) processes. One intuitive definition for LS processes, given in [Cardinali & Nason, 2010], is that their statistical properties change slowly over time. Alternatively, [Dahlhaus, 2012] defines them (informally) as processes which locally at each time point are close to a stationary process but whose characteristics (covariances, parameters, etc.) are gradually changing in an unspecific way as time evolves. A formal definition can be found in [Vogt, 2012], and [Dahlhaus, 2012] provides a rigorous review of the subject.\n",
    "\n",
    "LS processes are of importance because they somewhat bridge the gap between the thoroughly explored sub-class of parametric non-stationary processes (see the following section) and the uncharted waters of the wider family of non-parametric processes, in that they have received rigorous treatment and a corresponding set of analysis tools akin to those enjoyed by parametric processes. A great online resource on the topic is the home page of Prof. Guy Nason, who names LS processes as his main research interest.\n",
    "\n",
    "### The typology of notions of stationarity\n",
    "\n",
    "The following typology figure, partial as it may be, can help understand the relations between the different notions of stationarity we just went over:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "### Parametric notions of non-stationarity\n",
    "\n",
    "The definitions of stationarity presented so far have been non-parametric; i.e., they did not assume a model for the data-generating process, and thus apply to any stochastic process. The related concept of a difference stationarity and unit root processes, however, requires a brief introduction to stochastic process modeling.\n",
    "\n",
    "The topic of stochastic modeling is also relevant insofar as various simple models can be used to create stochastic processes (see figure 5).\n",
    "\n",
    "IMAGE\n",
    "\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f12478",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# ==================================================================\n",
    "\n",
    "## 2Ô∏è‚É£ Why is Stationarity Important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937eafb1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Many statistical and machine learning models assume that data is stationary. If the properties of the series change over time, it becomes harder to model or forecast accurately.\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Before diving into formal definitions of stationarity, and the related concepts upon which it builds, it is worth considering why the concept of stationarity has become important in time series analysis and its various applications.\n",
    "\n",
    "In the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time. It does not mean that the series does not change over time, just that the way it changes does not itself change over time. The algebraic equivalent is thus a linear function, perhaps, and not a constant one; the value of a linear function changes as ùíô grows, but the way it changes remains constant ‚Äî it has a constant slope; one value that captures that rate of change.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Why is this important? First, because stationary processes are easier to analyze. Without a formal definition for processes generating time series data (yet; they are called stochastic processes and we will get to them in a moment), it is already clear that stationary processes are a sub-class of a wider family of possible models of reality. This sub-class is much easier to model and investigate. The above informal definition also hints that such processes should be possible to predict, as the way they change is predictable.\n",
    "\n",
    "Although it sounds a bit streetlight effect-ish that simpler theories or models should become more prominent, it is actually quite a common pattern in science, and for good reason. In many cases simple models can be surprisingly useful, either as building blocks in constructing more elaborate ones, or as helpful approximations to complex phenomena. As it turns out, this also true for stationary processes.\n",
    "\n",
    "Due to these properties, stationarity has become a common assumption for many practices and tools in time series analysis. These include trend estimation, forecasting and causal inference, among others.\n",
    "\n",
    "The final reason, thus, for stationarity‚Äôs importance is its ubiquity in time series analysis, making the ability to understand, detect and model it necessary for the application of many prominent tools and procedures in time series analysis. Indeed, for many cases involving time series, you will find that you have to be able to determine if the data was generated by a stationary process, and possibly to transform it so it has the properties of a sample generated by such a process.\n",
    "\n",
    "Hopefully, I have convinced you by now that understanding stationarity is important if you want to deal with time series data, and we can proceed to introducing the subject more formally.\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Why Stationarity is a Big Deal\n",
    "\n",
    "Imagine your predictive models as expert navigators sailing through the sea of data. To navigate smoothly, they prefer calm waters ‚Äî that‚Äôs where stationarity comes in. Stationary data is like a serene ocean, where patterns stay consistent. But, if your data is a stormy sea with waves of ups and downs (non-stationary), accurate predictions become a real challenge. That‚Äôs why we need to spot these storms and transform our data into a peaceful pond for effective time series analysis.\n",
    "\n",
    "### Real-world Implications\n",
    "\n",
    "Data stationarity isn‚Äôt just a tech thing; it‚Äôs everywhere, influencing decisions from finance to predicting the weather. In finance, where precision is key for risk and return estimates, assuming stationarity is like having a reliable compass. Climate scientists rely on stationary models to predict long-term weather patterns ‚Äî it‚Äôs like having a trustworthy weather app for Earth‚Äôs future.\n",
    "\n",
    "### Journey to Insightful Analysis\n",
    "\n",
    "Getting our data stationary is more than a tech quest; it‚Äôs an adventure toward clarity. It‚Äôs like transforming a chaotic treasure map into a clear guide that helps analysts and decision-makers make sense of it all. In the dynamic world of time-dependent data, stationarity becomes our trusty map, guiding us to understand the patterns beneath the surface and making our journey through data waters much smoother.\n",
    "\n",
    "Alright, now that we get why it‚Äôs cool to have calm data, let‚Äôs learn how to make it chill. But wait, before we get our hands dirty by code, let me introduce you to something called ‚Äúunit roots.‚Äù Think of them as the special ingredients that affect how our data behaves. Knowing about unit roots is like having a secret recipe to turn our wavy, wild data into a smooth pond, ready for us to dive in and explore. So, get ready for the next part of our journey!\n",
    "\n",
    "(from Understanding Predictive Maintenance ‚Äî Unit Roots and Stationarity)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "The question still lies in why do we need to ensure our time series is stationary?\n",
    "\n",
    "Well, there are a few reasons:\n",
    "\n",
    "- Most forecasting model assume the data is stationary\n",
    "- Stationarity helps to make each data point independent\n",
    "- Makes the data, in general, easier to analyse\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Some time series forecasting models (e.g., autoregressive models) require a stationary time series because they are easier to model due to their constant statistical properties [3]. Thus, you should make your time series stationary if it is not (see What Can You Do When A Time Series Is Not Stationary?).\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "That‚Äôs clear now, but why do we need stationarity? 2 reasons (the most important ones), my friend:\n",
    "\n",
    "1. Stationary processes are easier to analyze\n",
    "2. Stationarity is assumed by most of the algorithms\n",
    "\n",
    "\n",
    "(from What is Stationarity in Time Series and why should you care)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "You should care about stationarity for two reasons:\n",
    "\n",
    "- Stationary processes are easier to analyze.\n",
    "- Most forecasting algorithms assume a series is stationary.\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Detecting stationarity in time series data\n",
    "\n",
    "Stationarity is an important concept in time series analysis. For a concise (but thorough) introduction to the topic, and the reasons that make it important, take a look at my previous blog post on the topic. Without reiterating too much, it is suffice to say that:\n",
    "\n",
    "1. Stationarity means that the statistical properties of a a time series (or rather the process generating it) do not change over time.\n",
    "2. Stationarity is important because many useful analytical tools and statistical tests and models rely on it.\n",
    "\n",
    "As such, the ability to determine wether a time series is stationary is important. Rather than deciding between two strict options, this usually means being able to ascertain, with high probability, that a series is generated by a stationary process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153e03c",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "## 2Ô∏è‚É£ What if the Data is Not Stationary?\n",
    "\n",
    "The figure below is a clear example of what non-stationary data looks like. The plot on the left has a strong positive trend with strong seasonality. Although this tells us a lot about the characteristics of the data, it is not stationary and therefore cannot be forecasted using traditional time series models. We need to transform the data in order to flatten the increasing variance.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Since the data is non-stationary, you could perform a transformation to convert into a stationary dataset. The most common transforms are the difference and logarithmic transform.\n",
    "\n",
    "(from Why Does Stationarity Matter in Time Series Analysis?)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## What Can You Do When A Time Series Is Not Stationary?\n",
    "\n",
    "You can apply different transformations to a non-stationary time series to try to make it stationary:\n",
    "\n",
    "- Differencing\n",
    "- Detrending by model fitting\n",
    "- Log transformation\n",
    "\n",
    "\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909c985",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# =======================================================================\n",
    "## 4Ô∏è‚É£ Detecting stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403990c",
   "metadata": {},
   "source": [
    "## Dickey-Fuller Test (DF)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "#### The Dickey-Fuller Test\n",
    "\n",
    "The Dickey-Fuller test was the first statistical test developed to test the null hypothesis that a unit root is present in an autoregressive model of a given time series, and that the process is thus not stationary. The original test treats the case of a simple lag-1 AR model. The test has three versions, that differ in the model of unit root process they test for;\n",
    "\n",
    "1. Test for a unit root: ‚àÜy·µ¢ = Œ¥y·µ¢‚Çã‚ÇÅ + u·µ¢\n",
    "2. Test for a unit root with drift: ‚àÜy·µ¢ = a‚ÇÄ + Œ¥y·µ¢‚Çã‚ÇÅ + u·µ¢\n",
    "3. Test for a unit root with drift and deterministic time trend:\n",
    "    ‚àÜy·µ¢ = a‚ÇÄ + a‚ÇÅ*t + Œ¥y·µ¢‚Çã‚ÇÅ + u·µ¢\n",
    "\n",
    "The choice of which version to use ‚Äî which can significantly effect the size and power of the test ‚Äî can use prior knowledge or structured strategies for series of ordered tests, allowing the discovery of the most fitting version.\n",
    "\n",
    "Extensions of the test were developed to accommodate more complex models and data; these include the Augmented Dickey-Fuller (ADF) (using AR of any order p and supporting modeling of time trends), the Phillips-Perron test (PP) (adding robustness to unspecified autocorrelation and heteroscedasticity)and the ADF-GLS test (locally de-trending data to deal with constant and linear trends).\n",
    "\n",
    "Python implementations can be found in the statsmodels and ARCH packages.\n",
    "\n",
    "(from Detecting stationarity in time series data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988ce5e",
   "metadata": {},
   "source": [
    "## Augmented Dickey-Fuller Test (ADF)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "### üìè Augmented Dickey-Fuller Test (ADF)\n",
    "\n",
    "* H‚ÇÄ: The series is **non-stationary**\n",
    "* H‚ÇÅ: The series is **stationary**\n",
    "\n",
    "```python\n",
    "def adf_test(ts, title=\"ADF Test\"):\n",
    "    print(f\"--- {title} ---\")\n",
    "    result = adfuller(ts)\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"Critical Value ({key}): {value:.4f}\")\n",
    "    print(\"Stationary\" if result[1] < 0.05 else \"Non-Stationary\")\n",
    "    print()\n",
    "\n",
    "adf_test(stationary_series, \"ADF Test - Stationary Series\")\n",
    "adf_test(random_walk, \"ADF Test - Random Walk\")\n",
    "```\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "There are several unit root tests you can use to check for stationarity. This article will focus on the most popular ones:\n",
    "\n",
    "- Augmented Dickey-Fuller test [2]\n",
    "- Kwiatkowski-Phillips-Schmidt-Shin test [4].\n",
    "\n",
    "### How to test for stationarity with Augmented Dickey-Fuller test\n",
    "\n",
    "The hypotheses for the Augmented Dickey-Fuller (ADF) test are [2]:\n",
    "\n",
    "1. Null hypothesis (H0): The time series is not stationary because there is a unit root (if p-value > 0.05)\n",
    "2. Alternative hypothesis (H1): The time series is stationary because there is no unit root (if p-value ‚â§ 0.05)\n",
    "\n",
    "In Python, we can use the adfuller method from the statsmodels.tsa.stattools library [8].\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df[\"example\"].values)\n",
    "\n",
    "The time series is stationary if we can reject the null hypothesis of the ADF test:\n",
    "\n",
    "- If the p-value (result[1]) ‚â§ 0.05\n",
    "- If the test statistic (result[0]) is more extreme than the critical value (result[4][\"1%\"], result[4][\"5%\"], and result[4][\"10%\"])\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Below are the results from the ADF test for the sample dataset:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Augmented Dickey-Fuller (ADF) Test\n",
    "\n",
    "Although the visual test is a quick-and-dirty method to detect stationary, most cases will not as easy as the one above. Statistical tests allow us to prove our hypothesis by testing for stationarity. The ADF test, also known as the ‚Äúunit root test‚Äù, is a statistical test to inform the degree to which a null hypothesis can be rejected or fail to be rejected. The p-value below a threshold (1% or 5%) suggests we reject the null hypothesis.\n",
    "\n",
    "    Null Hypothesis H0 = If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary\n",
    "\n",
    "    Alternative Hypothesis H1 = The null hypothesis is rejected and suggests the time series does not have a unit root, meaning it is stationary\n",
    "\n",
    "The easiest way to implement this test into your code is to use the adfuller() function in the statsmodels library.\n",
    "\n",
    "(from Why Does Stationarity Matter in Time Series Analysis?)\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def ADF_Cal(x):\n",
    "    result = adfuller(x)\n",
    "    ADF_stat = result[0]\n",
    "    p = result[1]\n",
    "    print(\"ADF Statistic: %f\" % ADF_stat)\n",
    "    print(\"p-value: %f\" % p)\n",
    "    print(\"Critical Values\")\n",
    "    levels = [.01, .05, .1]\n",
    "    i = 0\n",
    "    for key,value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key,value))\n",
    "        hyp = p < levels[i]\n",
    "        if ADF_stat < value:\n",
    "            cert = (1-levels[i])*100\n",
    "            print(\"{}% certain this is staionary\".format(cert))\n",
    "            print('Reject H0: {}'.format(hyp))\n",
    "            break\n",
    "        i = i+1\n",
    "        if i >= 3:\n",
    "            print(\"Less than 90% certain that data is stationary\")\n",
    "            print('Reject H0: {}'.format(hyp))print(\"Calculating ADF test for X...\")\n",
    "ADF_Cal(X)\n",
    "```\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Testing For Stationarity\n",
    "\n",
    "Visually, the data is now stationary. However, there are more quantitative techniques to determine if the data is indeed stationary.\n",
    "\n",
    "One such method is the Augmented Dickey-Fuller (ADF) test. This is a statistical hypothesis test where the null hypothesis is the series is non-stationary (also known as a unit root test).\n",
    "\n",
    "The statsmodels package provides an easy to use function for carrying out the ADF test:\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series):\n",
    "    \"\"\"Using an ADF test to determine if a series is stationary\"\"\"\n",
    "    test_results = adfuller(series)\n",
    "    print('ADF Statistic: ', test_results[0])\n",
    "    print('P-Value: ', test_results[1])\n",
    "    print('Critical Values:')\n",
    "    for thres, adf_stat in test_results[4].items():\n",
    "        print('\\t%s: %.2f' % (thres, adf_stat))\n",
    "\n",
    "\n",
    "adf_test(data[\"Passenger_Diff_Log\"][1:])\n",
    "```\n",
    "\n",
    "Running this function we get the following output:\n",
    "\n",
    "ADF Statistic: -2.717131\n",
    "P-Value: 0.071121\n",
    "Critical Values:\n",
    "        1%: -3.48\n",
    "        5%: -2.88\n",
    "        10%: -2.58\n",
    "\n",
    "Our ADF P-value (7.1%) is in-between the 5% and 10%, so depending on where you set your significance level we either reject or fail to reject the null hypothesis.\n",
    "\n",
    "We can perhaps carry out further differencing to make it even more stationary if we want.\n",
    "\n",
    "    If your interested in learning in-depth how the ADF test mathematically works, refer to the links I provided in the references section.\n",
    "\n",
    "The ADF test is not the only test available for stationarity, there is also the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test. However, in this test the null hypothesis is that the trend is stationary.\n",
    "\n",
    "    To learn more about the process of hypothesis testing, see the references section.\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "## Testing for Stationarity\n",
    "\n",
    "Some time back, two good fellas named David Dickey and Wayne Fuller have developed a test for stationarity. You would guess it, it‚Äôs called the Dicky-Fuller test, or DF test for short. Sometime later an improved version of the test was developed to take into account time dependencies, and it‚Äôs called the Augmented Dicky Fuller test (ADF-test).\n",
    "\n",
    "The entire test boils down to a simple hypothesis testing, where:\n",
    "\n",
    "- H0: Time series is not stationary\n",
    "- HA: Time series is stationary\n",
    "\n",
    "This means that we can easily calculate the test statistic and compare it to critical values. If the test statistic is lower than the critical value, we can reject the null hypothesis and declare time series as stationary.\n",
    "\n",
    "ADF-test from Python‚Äôs statsmodels library will return you the following:\n",
    "\n",
    "- Test-statistic\n",
    "- P-value\n",
    "- Number of lags used\n",
    "- {1%, 5%, and 10% critical values}\n",
    "- Estimation of the maximized information criteria (basically the lower it is the easier it is to make future forecasts)\n",
    "\n",
    "For simplicity's sake, I will compare test-statistic to the p-value, but you can, later on, compare it to 1% critical value if you want. Without further ado, let‚Äôs get started!\n",
    "\n",
    "## Imports and Dataset\n",
    "\n",
    "With regards to the libraries you‚Äôll need, two of them are the usual suspects ‚Äî Numpy and Pandas ‚Äî but you‚Äôll also need to import statoolsfrom the statsmodelslibrary:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "Now you can read in the dataset from the provided URL, and do some setup to make everything as it needs to be:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "If you‚Äôve done any time series analysis before, I‚Äôm sure you‚Äôre familiar with this dataset. For those who are not, this is how the first couple of rows look like:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "Let‚Äôs make a quick visualization also, just to eyeball if the time series is stationary by default:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Just with a quick look, it‚Äôs easy to determine the time series is not stationary. The average value changes over time and the peaks in the seasonal periods seem to get only larger.\n",
    "\n",
    "It would be nice, however, to determine stationarity analytically. That‚Äôs what the next section will cover.\n",
    "\n",
    "## Performing the ADF-Test\n",
    "\n",
    "Remember the import you did from the statsmodelslibrary? We‚Äôre gonna use it now to test for stationarity. The statoolscontains adfullermethod to which you can pass your time-series data:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "Well, the situation is not great. As expected, the time series isn‚Äôt stationary, which the p-value confirms (0.99). Let‚Äôs explore a method that will differentiate the series ‚Äî ergo subtract the current value by the previous one. The method is called diff(), and in it, you can pass the order ‚Äî default is 1:\n",
    "\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "After the same test is performed on the differentiated time series, you can see the p-value dropped just slightly above the usual significance level ‚Äî not quite satisfactory just yet.\n",
    "\n",
    "In a case you‚Äôre wondering why we‚Äôre dropping missing values, here‚Äôs the reason:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "As you can see, you cannot subtract from the first value, so that results in a missing value. ADF-Test will fail if time series with missing data is provided, so keep that in mind.\n",
    "\n",
    "We can easily use different differentiation orders to see if the p-value will drop. Let‚Äôs try with order = 2:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "The p-value is now below the significance level ‚Äî so the time series can be declared as stationary.\n",
    "\n",
    "Doing this entire process manually can be tedious ‚Äî even unmanageable if you have to deal with lots of time series data. Let‚Äôs imagine you want to automate some portion of time series model training ‚Äî this would be a great place to start if you‚Äôre gonna use algorithms that require stationary series.\n",
    "\n",
    "That‚Äôs the reason why I decided to make a function that will handle the process for you. I won‚Äôt explain it much here, as it is properly commented:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "Now the declared function can be easily used:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "And just quickly to verify the results ‚Äî we‚Äôll test for stationarity of supposedly stationary time series:\n",
    "\n",
    "CODE IMAGE\n",
    "\n",
    "Looks like everything is good, differentiation order is 2 (as calculated manually), and the time series is stationary ‚Äî by the p-value.\n",
    "\n",
    "(from What is Stationarity in Time Series and why should you care)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Augumented Dickey Fuller (ADF) helps us\n",
    "\n",
    "Imagine you have a line of ants moving in a certain direction. The ADF test checks if the ants are marching with a purpose (stationary) or if they‚Äôre randomly scattered all over the place (non-stationary).\n",
    "\n",
    "The ADF test involves a bit of math, but let‚Äôs simplify it:\n",
    "\n",
    "- Null Hypothesis (H0): This is like the default assumption. The null hypothesis for ADF is that the data has a unit root, which means it‚Äôs non-stationary. It‚Äôs like saying the ants are wandering randomly.\n",
    "    H0:The data has a unit root (non-stationary)\n",
    "- Alternative Hypothesis (H1): This is what we‚Äôre trying to prove. The alternative hypothesis is that the data is stationary, like the ants marching in a clear line.\n",
    "    H1:The data is stationary\n",
    "- Test Statistic: The ADF test calculates a number called the test statistic. If this number is very small, it suggests that the data is likely stationary.\n",
    "    P-value: This is a probability score. If the p-value is small (less than a certain threshold, like 0.05), we reject the null hypothesis and accept the alternative, saying our data is probably stationary.\n",
    "\n",
    "This is not very complicated, just run the tests and check P-value\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# Perform the Augmented Dickey-Fuller (ADF) test for stationarity\n",
    "adf_statistic, adf_p_value, adf_lags,\n",
    "adf_nobs, adf_critical_values, adf_reg_results = adfuller(stationary_series)\n",
    "\n",
    "# Check if the series is stationary based on the p-value\n",
    "is_stationary = adf_p_value < 0.05  # Using a significance level of 0.05\n",
    "```\n",
    "\n",
    "You will probably most of the time use adf like this:\n",
    "\n",
    "```python\n",
    "# What youy will probably will use most of the time\n",
    "_, adf_p_value, _, _, _, _= adfuller(stationary_series)\n",
    "```\n",
    "\n",
    "But I will explain what is behind these variables\n",
    "\n",
    "- adf_statistic: The test statistic from the ADF test, indicating the strength of evidence against the null hypothesis of non-stationarity.\n",
    "- adf_p_value: The p-value associated with the null hypothesis. A lower p-value suggests stronger evidence against non-stationarity.\n",
    "- adf_lags: The number of lags used in the test.\n",
    "- adf_nobs: The number of observations used in the ADF test.\n",
    "- adf_critical_values: The critical values for the test statistic at various significance levels.\n",
    "- adf_reg_results: The regression results, which provide additional information about the linear regression performed during the test.\n",
    "\n",
    "While chaos might seem daunting, we can transform it into our ally by understanding and harnessing its patterns. In the realm of data and analysis, chaos can be a powerful force that, when properly channeled, provides insights, predictions, and a clearer path forward. It‚Äôs all about turning the unpredictable into an advantage, making chaos our strategic companion in the journey of exploration and understanding.\n",
    "\n",
    "## Stationarity check\n",
    "\n",
    "First lets generate the series.\n",
    "\n",
    "```python\n",
    "# Generate series\n",
    "stationary_series_pseudorandom = generate_stationary_series_pseudorandom()\n",
    "stationary_series_random = generate_stationary_series_pseudorandom()\n",
    "\n",
    "\n",
    "titles = [\n",
    "    'stationary_series_pseudorandom',\n",
    "    'stationary_series_random'\n",
    "]\n",
    "\n",
    "plot_multiple_series(stationary_series_random, stationary_series_pseudorandom, \n",
    "                     titles=titles)\n",
    "\n",
    "\n",
    "_, adf_p_value, _, _, _, _= adfuller(stationary_series_pseudorandom)\n",
    "print(f'PseudoRandom adf p-value: {adf_p_value}')\n",
    "_, adf_p_value, _, _, _, _= adfuller(stationary_series_random)\n",
    "print(f'TrueRandom adf p-value: {adf_p_value}')\n",
    "```\n",
    "\n",
    "When the p-value is very small (<0.05), it provides evidence against the null hypothesis, suggesting that your data is likely stationary.\n",
    "\n",
    "So, in this case, with a p-value much smaller than 0.05, you have the confidence to say, ‚ÄúYes, our data is stationary.‚Äù\n",
    "\n",
    "Now, let‚Äôs take a moment to crunch the numbers. Our pseudorandom boasts a P-value approximately 2 million times smaller than the truly random one.\n",
    "\n",
    "Why does this happen? Pseudorandom numbers are generated by algorithms, introducing a level of determinism. These algorithms can unintentionally introduce patterns or structure into the data. On the other hand, truly random data, like atmospheric noise, is more likely to exhibit the characteristics of pure randomness. The ADF test, keen on detecting patterns indicative of non-stationarity, may find less evidence of such patterns in truly random data, leading to a relatively higher P-value.\n",
    "\n",
    "(from Understanding Predictive Maintenance ‚Äî Unit Roots and Stationarity)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## ADF test ‚Äî How to test for stationarity\n",
    "\n",
    "A while back, David Dickey and Wayne Fuller developed a test for stationarity ‚Äî Dicky-Fuller test. It was improved later and renamed to Augmented Dicky-Fuller test, or ADF test for short.\n",
    "\n",
    "It boils down to a simple hypothesis testing:\n",
    "\n",
    "- Null hypothesis (H0) ‚Äî Time series is not stationary.\n",
    "- Alternative hypothesis (H1) ‚Äî Time series is stationary.\n",
    "\n",
    "In Python, the ADF test returns the following:\n",
    "\n",
    "- Test statistic\n",
    "- P-value\n",
    "- Number of lags used\n",
    "- 1%, 5%, and 10% critical values\n",
    "- Estimation of the maximized information criteria (don‚Äôt worry about it)\n",
    "\n",
    "If the returned P-value is higher than 0.05, the time series isn‚Äôt stationary. 0.05 is the standard threshold, but you‚Äôre free to change it.\n",
    "\n",
    "Let‚Äôs implement the ADF test next. We‚Äôll start with the library imports, dataset loading, and visualization for the airline passengers dataset:\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from cycler import cycler\n",
    "\n",
    "rcParams['figure.figsize'] = 18, 5\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.prop_cycle'] = cycler(color=['#365977'])\n",
    "rcParams['lines.linewidth'] = 2.5\n",
    "\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv('data/airline-passengers.csv', index_col='Month', parse_dates=True)\n",
    "\n",
    "# Visualize\n",
    "plt.title('Airline Passengers dataset', size=20)\n",
    "plt.plot(df);\n",
    "```\n",
    "\n",
    "\n",
    "Here‚Äôs how the dataset looks like:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "It doesn‚Äôt look stationary at all, but let‚Äôs verify that with a test:\n",
    "\n",
    "# ADF stationarity test\n",
    "# Returns: {Test statistic, P-value, Num lags used, {Critical values}, Estmation of maximized information criteria}\n",
    "adfuller(df['Passengers'])\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf1154",
   "metadata": {},
   "source": [
    "## KPSS Test\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "### üìè KPSS Test (opposite hypothesis)\n",
    "\n",
    "* H‚ÇÄ: The series is **stationary**\n",
    "* H‚ÇÅ: The series is **non-stationary**\n",
    "\n",
    "```python\n",
    "def kpss_test(ts, title=\"KPSS Test\"):\n",
    "    print(f\"--- {title} ---\")\n",
    "    statistic, p_value, lags, crit = kpss(ts, regression='c', nlags=\"auto\")\n",
    "    print(f\"KPSS Statistic: {statistic:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    for key, value in crit.items():\n",
    "        print(f\"Critical Value ({key}): {value:.4f}\")\n",
    "    print(\"Non-Stationary\" if p_value < 0.05 else \"Stationary\")\n",
    "    print()\n",
    "\n",
    "kpss_test(stationary_series, \"KPSS Test - Stationary Series\")\n",
    "kpss_test(random_walk, \"KPSS Test - Random Walk\")\n",
    "```\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "There are several unit root tests you can use to check for stationarity. This article will focus on the most popular ones:\n",
    "\n",
    "- Augmented Dickey-Fuller test [2]\n",
    "- Kwiatkowski-Phillips-Schmidt-Shin test [4].\n",
    "\n",
    "### How to test for stationarity with Kwiatkowski-Phillips-Schmidt-Shin test\n",
    "\n",
    "The hypotheses for the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test are [4]:\n",
    "\n",
    "1. Null hypothesis (H0): The time series is stationary because there is no unit root (if p-value > 0.05)\n",
    "2. Alternative hypothesis (H1): The time series is not stationary because there is a unit root (if p-value ‚â§ 0.05)\n",
    "\n",
    "The more positive this statistic, the more likely we are to reject the null hypothesis (we have a non-stationary time series).\n",
    "\n",
    "In Python, we can use the kpss method from the statsmodels.tsa.stattools library [9]. We must use the argument regression = 'ct' to specify that the test's null hypothesis is that the data is trend stationary. [9]\n",
    "\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "result = kpss(df[\"example\"].values, \n",
    "              regression = \"ct\")\n",
    "\n",
    "The time series is stationary if we fail to reject the null hypothesis of the KPSS test:\n",
    "\n",
    "- If the p-value (result[1]) > 0.05\n",
    "- If the test statistic (result[0]) is less extreme than the critical value (result[3][\"1%\"], result[3][\"2.5%\"], result[3][\"5%\"], and result[3][\"10%\"])\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Below are the results from the KPSS test for the sample dataset:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "#### The KPSS Test\n",
    "\n",
    "Another prominent test for the presence of a unit root is the KPSS test. [Kwiatkowski et al, 1992] Conversely to the Dickey-Fuller family of tests, the null hypothesis assumes stationarity around a mean or a linear trend, while the alternative is the presence of a unit root.\n",
    "\n",
    "The test is based on linear regression, breaking up the series into three parts: a deterministic trend (Œ≤t), a random walk (rt), and a stationary error (Œµt), with the regression equation:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "and where u~(0,œÉ¬≤) and are iid. The null hypothesis is thus stated to be H‚ÇÄ: œÉ¬≤=0 while the alternative is H‚Çê: œÉ¬≤>0. Whether the stationarity in the null hypothesis is around a mean or a trend is determined by setting Œ≤=0 (in which case x is stationary around the mean r‚ÇÄ) or Œ≤‚â†0, respectively.\n",
    "\n",
    "The KPSS test is often used to complement Dickey-Fuller-type tests. I will touch on how to interpret such combined results in a future post.\n",
    "\n",
    "Python implementations can be found in the statsmodels and ARCH packages.\n",
    "\n",
    "(from Detecting stationarity in time series data)\n",
    "\n",
    "# ============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3408cf",
   "metadata": {},
   "source": [
    "## Visually\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "You can test for stationarity with statistical tests, but sometimes plotting a time series can give you a rough estimate. Here‚Äôs an image showing stationary vs. non-stationary series:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "A stationary series is centered around some value, doesn‚Äôt have too many spikes and unexpected variations, and doesn‚Äôt show drastic behavior changes from one part to the other.\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "## How to visually assess stationarity\n",
    "\n",
    "You can visually assess the stationarity of a time series by mentally dividing the time series in half and comparing the mean, amplitude, and cycle length from the first half to the second half of the time series.\n",
    "\n",
    "- constant mean ‚ÄîThe mean value of the first half of the time series should be similar to that of the second half.\n",
    "- constant variance ‚ÄîThe amplitude of the first half of the time series should be similar to that of the second half.\n",
    "- covariance is independent of time ‚Äî The cycle length in the first half of the time series should be similar to that in the second half. The cycles should be independent on time (e.g., not weekly or monthly, etc.).\n",
    "\n",
    "IMAGE\n",
    "\n",
    "For our examples, the assessment result is visualized below:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "The most basic methods for stationarity detection rely on plotting the data, or functions of it, and determining visually whether they present some known property of stationary (or non-stationary) data.\n",
    "\n",
    "### Looking at the data\n",
    "\n",
    "Trying to determine whether a time series was generated by a stationary process just by looking at its plot is a dubious venture. However, there are some basic properties of non-stationary data that we can look for. Let‚Äôs take as example the following nice plots from [Hyndman & Athanasopoulos, 2018]:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "[Hyndman & Athanasopoulos, 2018] give several heuristics used to rule out stationarity in the above plots, corresponding to the basic characteristic of stationary processes (which we‚Äôve discussed previously):\n",
    "\n",
    "- Prominent seasonality can be observed in series (d), (h) and (i).\n",
    "- Noticeable trends and changing levels can be seen in series (a), (c), (e), (f) and (i).\n",
    "- Series (i) shows increasing variance.\n",
    "\n",
    "The authors also add that although the strong cycles in series (g) might appear to make it non-stationary, the timing of these cycles makes them unpredictable (due to the underlying dynamic dominating lynx population, driven partially by available feed). This leaves series (b) and (g) as the only stationary series.\n",
    "\n",
    "If, like me, you didn‚Äôt find at least some of these observations trivial to make by looking at the above figure, you are not the only one. Indeed, this is not a very dependable method to detect stationarity, and it is usually used to get an initial impression of the data rather than to make definite assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3628991",
   "metadata": {},
   "source": [
    "## How Do You Test For Stationarity?\n",
    "\n",
    "You can test a time series for stationarity in two ways:\n",
    "\n",
    "1. Intuitive approach: Visual assessment\n",
    "2. Statistical approach: Unit root test\n",
    "\n",
    "For this section, we will recreate a few examples Hyndman and Athanasopoulos [3] used to explain the visual assessment of stationarity and extend their usage also to explain testing for stationarity with unit root testing. The data is taken from the related fma R-package [1].\n",
    "\n",
    "IMAGE\n",
    "\n",
    "\n",
    "## How to statistically assess stationarity ‚Äî a detour on unit root tests\n",
    "\n",
    "A unit root is a stochastic trend called a ‚Äúrandom walk with drift‚Äù. Since randomness can‚Äôt be predicted, that means:\n",
    "\n",
    "- Unit root present: not stationary (unpredictable)\n",
    "- Unit root absent: stationary\n",
    "\n",
    "To test for stationarity with a unit root test, you will state your initial assumption in the form of two competing hypotheses [6]:\n",
    "\n",
    "- Null hypothesis (H0) ‚Äî e.g., the time series is stationary (no unit root present)\n",
    "- Alternative hypothesis (H1) ‚Äî e.g., the time series is not stationary (unit root present)\n",
    "\n",
    "Then you will assess whether to reject or not to reject the null hypothesis based on two approaches:\n",
    "\n",
    "- The p-value approach:\n",
    "    If the p-value > 0.05, fail to reject the null hypothesis.\n",
    "    If the p-value ‚â§ 0.05, reject the null hypothesis.\n",
    "- The critical value approach:\n",
    "    If the test statistic is less extreme than the critical value, fail to reject the null hypothesis.\n",
    "    If the test statistic is more extreme than the critical value, reject the null hypothesis.\n",
    "    The critical value approach should be used when the p-value is close to significant (e.g., around 0.05) [8].\n",
    "\n",
    "There are several unit root tests you can use to check for stationarity. This article will focus on the most popular ones:\n",
    "\n",
    "- Augmented Dickey-Fuller test [2]\n",
    "- Kwiatkowski-Phillips-Schmidt-Shin test [4].\n",
    "\n",
    "(moved from here)\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612cfe5",
   "metadata": {},
   "source": [
    "## Stationarity and ACF Plots\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Looking at Autocorrelation Function (ACF) plots\n",
    "\n",
    "Autocorrelation is the correlation of a signal with a delayed copy ‚Äî or a lag ‚Äî of itself as a function of the delay. When plotting the value of the ACF for increasing lags (a plot called a correlogram), the values tend to degrade to zero quickly for stationary time series (see figure 1, right), while for non-stationary data the degradation will happen more slowly (see figure 1, left).\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Alternatively, [Nielsen, 2006] suggests that plotting correlograms based on both autocorrelations and scaled autocovariances, and comparing them, provides a better way of discriminating between stationary and non-stationary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f45c5",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "\n",
    "## Parametric tests\n",
    "\n",
    "Another, more rigorous approach, to detecting stationarity in time series data is using statistical tests developed to detect specific types of stationarity, namely those brought about by simple parametric models of the generating stochastic process (see my previous post for details).\n",
    "\n",
    "I‚Äôll present here the most prominent tests. I‚Äôll also name Python implementations for each test, assuming I have found any. For R implementations see the CRAN Task View: Time Series Analysis (also here).\n",
    "\n",
    "## Unit root tests\n",
    "\n",
    "#### The Dickey-Fuller Test\n",
    "(moved from here)\n",
    "\n",
    "#### The KPSS Test\n",
    "(moved from here)\n",
    "\n",
    "\n",
    "#### The Zivot and Andrews Test\n",
    "\n",
    "The aforementioned tests do not allow for the possibility of a structural break ‚Äî an abrupt change involving a change in the mean or other parameters of the process. Assuming the time of the break as an exogenous phenomenon, Perron showed that the power to reject a unit root decreases when the stationary alternative is true and a structural break is ignored.\n",
    "\n",
    "[Zivot and Andrews, 1992] propose a unit root test in which they assume that the exact time of the break-point is unknown. Following Perron‚Äôs characterization of the form of structural break, Zivot and Andrews proceed with three models to test for a unit root:\n",
    "\n",
    "- Model A: Permits a one-time change in the level of the series.\n",
    "- Model B: Allows for a one-time change in the slope of the trend function.\n",
    "- Model C: Combines one-time changes in the level and the slope of the trend function of the series.\n",
    "\n",
    "Hence, to test for a unit root against the alternative of a one-time structural break, Zivot and Andrews use the following regression equations corresponding to the above three models: [Waheed et al, 2006]\n",
    "\n",
    "EQUATIONS\n",
    "\n",
    "A Python implementation can be found in the ARCH package and here.\n",
    "\n",
    "## Semi-parametric unit root tests\n",
    "\n",
    "#### Variance Ratio Test\n",
    "\n",
    "[Breitung, 2002] suggested a non-parametric test for the presence of a unit root based on a variance ratio statistic. The null hypothesis is a process I(1) (integrated of order one) while the alternative is I(0). I list this test as semi-parametric because it tests for a specific, model-based, notion of stationarity.\n",
    "\n",
    "## Non-parametric tests\n",
    "\n",
    "In the wake of the limitations of parametric tests, and the recognition they cover only a narrow sub-class of possible cases encountered in real data, a class of non-parametric tests for stationarity has emerged in time series analysis literature.\n",
    "\n",
    "Naturally, these tests open up a promising avenue for investigating time series data: you no longer have to assume very simple parametric models happen to apply to your data to find out whether it is stationary or not, or risk not discovering a complex form of the phenomenon not captured by these models.\n",
    "\n",
    "The reality of it, however, is more complex; there aren‚Äôt, at the moment, any widely-applicable non-parametric tests that encompass all real-life scenarios generating time series data. Instead, these tests limit themselves to specific types of data or processes. Also, I was not able to find implementations for any of the following tests.\n",
    "\n",
    "I‚Äôll mention here the few that I have encountered:\n",
    "\n",
    "### A Nonparametric Test for Stationarity in Continuous-Time Markov Processes\n",
    "\n",
    "[Kanaya, 2011] suggest this nonparametric test stationarity for univariate time-homogeneous Markov processes only, construct a kernel-based test statistic and conduct Monte-Carlo simulations to study the finite-sample size and power properties of the test.\n",
    "\n",
    "### A nonparametric test for stationarity in functional time series\n",
    "\n",
    "[Delft et al, 2017] suggest a nonparametric stationarity test limited to functional time series ‚Äî data obtained by separating a continuous (in nature) time record into natural consecutive intervals, for example days. Note that [Delft and Eichler, 2018] have proposed a test for local stationarity for functional time series (see my previous post for some references on local stationarity). Also, [Vogt & Dette, 2015] suggest a nonparametric method to estimate a smooth change point in a locally stationary framework.\n",
    "\n",
    "### A nonparametric test for stationarity based on local Fourier analysis\n",
    "\n",
    "[Basu et al, 2009] suggest what may be the most applicable nonparametric test for stationarity present here, as it is applicable to any zero-mean discrete-time random process (and I assume here any finite sample of a discrete process you may have can easily be transformed to have zero mean).\n",
    "\n",
    "## Final Words\n",
    "\n",
    "That‚Äôs it. I hope the above review gave you some idea as to how to approach the issue of detecting stationarity in your data. I also hope that it exposed you to the complexities of this task; due to the lack of implementations to the handful of nonparametric tests out there, you will be forced to make strong assumptions about your data, and interpret the results you get with the required amount of doubt.\n",
    "\n",
    "As to the question of what to do once you have detected some type of stationarity in your data, I hope to touch on this in a future post. As always, I‚Äôd love to hear about things I‚Äôve missed or was wrong about. Cheers!\n",
    "\n",
    "## References\n",
    "\n",
    "## Academic literature\n",
    "\n",
    "- Basu, P., Rudoy, D., & Wolfe, P. J. (2009, April). A nonparametric test for stationarity based on local Fourier analysis. In 2009 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 3005‚Äì3008). IEEE.\n",
    "- Breitung, J. (2002). Nonparametric tests for unit roots and cointegration. Journal of econometrics, 108(2), 343‚Äì363.\n",
    "- Cardinali, A., & Nason, G. P. (2018). Practical powerful wavelet packet tests for second-order stationarity. Applied and Computational Harmonic Analysis, 44(3), 558‚Äì583.\n",
    "- Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: principles and practice. OTexts.\n",
    "- Kanaya, S. (2011). A nonparametric test for stationarity in continuous & time markov processes. Job Market Paper, University of Oxford.\n",
    "- Kwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?. Journal of econometrics, 54(1‚Äì3), 159‚Äì178.\n",
    "- Nielsen, B. (2006). Correlograms for non‚Äêstationary autoregressions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(4), 707‚Äì720.\n",
    "- Waheed, M., Alam, T., & Ghauri, S. P. (2006). Structural breaks and unit root: evidence from Pakistani macroeconomic time series. Available at SSRN 963958.\n",
    "- van Delft, A., Characiejus, V., & Dette, H. (2017). A nonparametric test for stationarity in functional time series. arXiv preprint arXiv:1708.05248.\n",
    "- van Delft, A. and Eichler, M. (2018). ‚ÄúLocally stationary functional time series.‚Äù Electronic Journal of Statistics, 12:107‚Äì170.\n",
    "- Vogt, M., & Dette, H. (2015). Detecting gradual changes in locally stationary processes. The Annals of Statistics, 43(2), 713‚Äì740.\n",
    "- Zivot, E. and D. Andrews, (1992), Further evidence of great crash, the oil price shock and unit root hypothesis, Journal of Business and Economic Statistics, 10, 251‚Äì270.\n",
    "\n",
    "## Online references\n",
    "\n",
    "- Data transformations and forecasting models: what to use and when\n",
    "- Forecasting Flow Chart\n",
    "- Documentation of the egcm R package\n",
    "- ‚ÄúNon-Stationary Time Series and Unit Root Tests‚Äù by Heino Bohn Nielsen\n",
    "- How to interpret Zivot & Andrews unit root test?\n",
    "\n",
    "## Comments:\n",
    "\n",
    "Mohsen Mollayi\n",
    "\n",
    "Mohsen Mollayi\n",
    "\n",
    "Oct 14, 2019\n",
    "\n",
    "This leaves series (b) as the only stationary series.\n",
    "\n",
    "Actually the authors claim that series (g) is also stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab2a0a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# =======================================================\n",
    "## 5Ô∏è‚É£ Stochastic Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd44c0",
   "metadata": {},
   "source": [
    "https://medium.com/data-science/stationarity-in-time-series-analysis-90c94f27322\n",
    "\n",
    "# Stationarity in time series analysis\n",
    "\n",
    "\n",
    "## A formal definition for stochastic processes\n",
    "\n",
    "Before introducing more formal notions for stationarity, a few precursory definitions are required. This section is meant to provide a quick overview of basic concepts in time series analysis and stochastic process theory required for further reading. Feel free to skip ahead if you are familiar with them.\n",
    "\n",
    "Time series: Commonly, a time series (x‚ÇÅ, ‚Ä¶, x‚Çë) is assumed to be a sequence of real values taken at successive equally spaced‚Å∂ points in time, from time t=1 to time t=e.\n",
    "\n",
    "Lag: For some specific time point r, the observation x·µ£‚Çã·µ¢ (i periods back) is called the i-th lag of x·µ£. A time series Y generated by back-shifting another time series X by i time steps is also sometime called the i-th lag of X, or an i-lag of X. This transformation is called both the backshifting operator, commonly denoted as B(‚àô),and the lag operator, commonly denoted as L(‚àô); thus, L(X·µ£)=X·µ£‚Çã‚ÇÅ. Powers of the operators are defined as L‚Å±(X·µ£)=X·µ£‚Çã·µ¢.\n",
    "\n",
    "## Stochastic Processes\n",
    "\n",
    "A common approach in the analysis of time series data is to consider the observed time series as part of a realization of a stochastic process. Two cursory definitions are required before defining stochastic processes.\n",
    "\n",
    "Probability Space: A probability space is a triple (Œ©, F, P), where\n",
    "(i) Œ© is a nonempty set, called the sample space.\n",
    "(ii) F is a œÉ-algebra of subsets of Œ©, i.e. a family of subsets closed with respect to countable union and complement with respect to Œ©.\n",
    "(iii) P is a probability measure defined for all members of F.\n",
    "\n",
    "Random Variable: A real random variable or real stochastic variable on (Œ©,F,P) is a function x:Œ©‚Üí‚Ñù, such that the inverse image of any interval (-‚àû,a] belongs to F; i.e. a measurable function.\n",
    "\n",
    "We can now define what is a stochastic process.\n",
    "\n",
    "Stochastic Process: A real stochastic process is a family of real random variables ùëø={x·µ¢(œâ); i‚ààT}, all defined on the same probability space (Œ©, F, P). The set T is called the index set of the process. If T‚äÇ‚Ñ§, then the process is called a discrete stochastic process. If T is an interval of ‚Ñù, then the process is called a continuous stochastic process.\n",
    "\n",
    "Finite Dimensional Distribution: For a finite set of integers T={t‚ÇÅ, ‚Ä¶,tn}, the joint distribution function of ùëø={X·µ¢(œâ); i‚ààT} is defined by\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Which for a stochastic process ùëø is also commonly denoted as:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "The finite dimensional distribution of a stochastic process is then defined to be the set of all such joint distribution functions for all such finite integer sets T of any size n. For a discrete process it is thus the set:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Intuitively, this represents a projection of the process onto a finite-dimensional vector space (in this case, a finite set of time points).\n",
    "\n",
    "### Basic concepts in stochastic process modeling\n",
    "\n",
    "The forecasting of future values is a common task in the study of time series data. To make forecasts, some assumptions need to be made regarding the Data Generating Process (DGP), the mechanism generating the data. These assumptions often take the form of an explicit model of the process, and are also often used when modeling stochastic processes for other tasks, such as anomaly detection or causal inference. We will go over the three most common such models.\n",
    "\n",
    "The autoregressive (AR) model: A time series modeled using an AR model is assumed to be generated as a linear function of its past values, plus a random noise/error:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "This is a memory-based model, in the sense that each value is correlated with the p preceding values; an AR model with lag p is denoted with AR(p). The coefficients ùúô·µ¢ are weights measuring the influence of these preceding values on the value x[t], c is constant intercept and Œµ·µ¢ is a univariate white noise process (commonly assumed to be Gaussian).\n",
    "\n",
    "The vector autoregressive (VAR) model generalizes the univariate case of the AR model to the multivariate case; now each element of the vector x[t] of length k can be modeled as a linear function of all the elements of the past p vectors:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "where c is a vector of k constants (the intercepts), A·µ¢ are time-invariant k√ók matrices and e={e·µ¢ ; i‚àà‚Ñ§} is a white noise multivariate process of k variables.\n",
    "\n",
    "The moving average (MA) model: A time series modeled using a moving average model, denoted with MA(q), is assumed to be generated as a linear function of the last q+1 random shocks generated by Œµ·µ¢, a univariate white noise process:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Like for autoregressive models, a vector generalization, VMA, exists.\n",
    "\n",
    "The autoregressive moving average (ARMA) model: A time series modeled using an ARMA(p,q) model is assumed to be generated as a linear function of the last p values and the last q+1 random shocks generated by Œµ·µ¢, a univariate white noise process:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "The ARMA model can be generalized in a variety of ways, for example to deal with non-linearity or with exogenous variables, to the multivariate case (VARMA) or to deal with (a specific type of) non-stationary data (ARIMA).\n",
    "\n",
    "### Difference stationary processes\n",
    "\n",
    "With a basic understanding of common stochastic process models, we can now discuss the related concept of difference stationary processes and unit roots. This concept relies on the assumption that the stochastic process in question can be written as an autoregressive process of order p, denoted as AR(p):\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Where Œµ·µ¢ are usually uncorrelated white-noise processes (for all times t). We can write the same process as:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "The part inside the parenthesis on the left is called the characteristic equation of the process. We can consider the roots of this equation:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "If m=1 is a root of the equation then the stochastic process is said to be a difference stationary process, or integrated. This means that the process can be transformed into a weakly-stationary process by applying a certain type of transformation to it, called differencing.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Difference stationary processes have an order of integration, which is the number of times the differencing operator must be applied to it in order to achieve weak stationarity. A process that has to be differenced r times is said to be integrated of order r, denoted by I(r). This coincides exactly with the multiplicity of the root m=1; meaning, if m=1 is a root of multiplicity r of the characteristic equation, then the process is integrated of order r.\n",
    "\n",
    "(from Stationarity in time series analysis)\n",
    "\n",
    "## Unit root processes\n",
    "\n",
    "A common sub-type of difference stationary process are processes integrated of order 1, also called unit root process. The simplest example for such a process is the following autoregressive model:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Unit root processes, and difference stationary processes generally, are interesting because they are non-stationary processes that can be easily transformed into weakly stationary processes. As a result, while the term is not used interchangeably with non-stationarity, the questions regarding them sometimes are.\n",
    "\n",
    "I thought it worth mentioning here, as sometime tests and procedures to check whether a process has a unit root (a common example is the Dickey-Fuller test) are mistakenly thought of as procedures for testing non-stationarity (as a latter post in this series touches upon). It is thus important to remember that these are distinct notions, and that while every process with a unit root is non-stationary, and so is every processes integrated to an order r>1, the opposite is far from true.\n",
    "\n",
    "### Semi-parametric unit root processes\n",
    "\n",
    "Another definition of interest is a wider, and less parametric, sub-class of non-stationary processes, which can be referred to as semi-parametric unit root processes. The definition was introduced in [Davidson, 2002], but a concise overview of it can be found [Breitung, 2002].\n",
    "\n",
    "If you are interested in the concept of stationarity, or have stumbled into the topic while working with time series data, then I hope you have found this post a good introduction to the subject. Some references and useful links are found below.\n",
    "\n",
    "As I have mentioned, a latter post in this series provides a similar overview of methods of detection of non-stationarity, and another will provide the same for transformation of non-stationarity time series data.\n",
    "\n",
    "Also, please feel free to get in touch with me with any comments and thoughts on the post or the topic.\n",
    "\n",
    "## References\n",
    "## Academic Literature\n",
    "\n",
    "- [Boshnakov, 2011] G. Boshnakov. On First and Second Order Stationarity of Random Coefficient Models. Linear Algebra Appl. 434, 415‚Äì423. 2011.\n",
    "- [Breitung, 2002] Breitung, J. (2002). Nonparametric tests for unit roots and cointegration. Journal of econometrics, 108(2), 343‚Äì363.\n",
    "- [Cardinali & Nason, 2010] Cardinali, A., & Nason, G. P. (2010). Costationarity of locally stationary time series. Journal of Time Series Econometrics, 2(2).\n",
    "- [Cox & Miller, 1965] Cox, D. R.; and Miller, H. D., 1965, The Theory of Stochastic Processes: Methuen, London, 398 p.\n",
    "- [Dahlhaus, 2012] Dahlhaus, R. (2012). Locally stationary processes. In Handbook of statistics (Vol. 30, pp. 351‚Äì413). Elsevier.\n",
    "- [Davidson, 2002] Davidson, J., 2002. Establishing conditions for the functional central limit theorem in nonlinear and semiparametric time series processes, Journal of Econometrics 106, 243‚Äì269.\n",
    "- [Dyrhovden, 2016] Dyrhovden, Sigve Brix. 2016. Stochastic unit-root processes. The University of Bergen.\n",
    "- [Fischer et al. 1996] Fischer, M. Scholten, H. J. and Unwin, D. Editors. Spatial analytical perspectives on GIS. Bristol, PA : Taylor & Francis, ‚Äî GISDATA ; 4.\n",
    "- [Myers, 1989] Myers, D.E., 1989. To be or not to be . . . stationary? That is the question. Math. Geol. 21, 347‚Äì362.\n",
    "- [Nason, 2006] Nason, GP 2006, Stationary and non-stationary time series. in H Mader & SC Coles (eds), Statistics in Volcanology. The Geological Society, pp. 129‚Äì142.\n",
    "- [Vogt, 2012] Vogt, M. (2012). Nonparametric regression for locally stationary time series. The Annals of Statistics, 40(5), 2601‚Äì2633.\n",
    "\n",
    "## Online References\n",
    "\n",
    "- A Gentle Introduction to Handling a Non-Stationary Time Series in Python at Analytics Vidhya\n",
    "- Unit Root at Wikipedia\n",
    "- Lesson 4: Stationary stochastic processes from Umberto Triacca‚Äôs course on stochastic processes\n",
    "- Roots of characteristic equation reciprocal to roots of its inverse\n",
    "- Stochastic Process Characteristics\n",
    "- Trend-Stationary vs. Difference-Stationary Processes\n",
    "- The home page of Prof. Guy Nason\n",
    "\n",
    "## Footnotes\n",
    "\n",
    "1. The phrasing here is not strictly accurate, since ‚Äî as we will soon see ‚Äî time series cannot be stationary themselves, rather only the processes generating them can. I have used it, however, so as not to assume any knowledge for the opening paragraphs. ‚Ü∫\n",
    "2. The common synonym of weak-sense stationarity as second order stationarity is probably related to (but should not be confused with) the concept of second order stochastic processes, which are defined as stochastic processes that has a finite second moment (i.e. variance). ‚Ü∫\n",
    "3. Note that the opposite is not true. Not every stationary process is composed of IID variables; Stationarity means that the joint distribution of variables doesn‚Äôt depend on time, but they may still depend on each other. ‚Ü∫\n",
    "4. This is also a good example for the fact that IID does not imply weak stationarity; since it does imply strong stationarity, however, it has the same necessary and sufficient condition for it to imply strong stationarity: having finite moments. ‚Ü∫\n",
    "5. One minor but interesting notion of stationarity is p-stationary processes.\n",
    "6. There are also formal ways to treat times series whose samples are not equally spaced.‚Ü∫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b782a27",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# =======================================================\n",
    "## 5Ô∏è‚É£ Unit Root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fea65",
   "metadata": {},
   "source": [
    "https://medium.com/data-science/understanding-predictive-maintenance-unit-roots-and-stationarity-f05322f7b6df\n",
    "\n",
    "# Understanding Predictive Maintenance ‚Äî Unit Roots and Stationarity\n",
    "\n",
    "## Article Purpose\n",
    "\n",
    "In this article, we‚Äôre diving into the critical concepts of unit roots and stationarity. Buckle up for an exploration into why checking stationarity is crucial, what unit roots are, and how these elements play a key role in our predictive maintenance arsenal. We will also master the chaos!\n",
    "This article is part of the series Understanding Predictive Maintenance. I plan to create the entire series in a similar style.\n",
    "\n",
    "Check the whole series in this link. Ensure you don‚Äôt miss out on new articles by following me.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Unit Roots ‚Äî Mischievous Time Travelers in Data‚Äôs History Book\n",
    "\n",
    "Unit roots are fundamental concepts in time series analysis, playing a pivotal role in understanding the behavior and characteristics of real-world data. In this exploration, we‚Äôll delve into what unit roots are, why they are important in real data analysis, and how they influence the predictive maintenance landscape. Of course, we will do some experiments in the hands-on section.\n",
    "\n",
    "## What are unit roots?\n",
    "\n",
    "A unit root in a time series variable implies a stochastic process where the variable‚Äôs value at any given time is influenced by its past values. Formally, a unit root suggests non-stationarity, indicating that the variable does not revert to a constant mean over time.\n",
    "\n",
    "EQUATIONS\n",
    "\n",
    "The presence of unit roots introduces persistence into the time series, leading to challenges in modeling and forecasting. The Augmented Dickey-Fuller (ADF) test and other statistical methods are employed to detect the existence of unit roots, providing a quantitative measure of non-stationarity.\n",
    "\n",
    "Unit roots are like the storytellers of our data, weaving narratives that extend beyond individual moments and create a continuous storyline. They signify the persistence of historical influences, introducing an element of memory into the numerical fabric of our datasets.\n",
    "\n",
    "Imagine your dataset as a historical novel, with each data point representing a chapter in the unfolding tale. Unit roots, in this context, are the recurring motifs and characters that leave an indelible mark on the narrative, guiding the plot with a subtle yet consistent influence.\n",
    "\n",
    "## Why it is important for us?\n",
    "\n",
    "Understanding unit roots is fundamental for time series analysts and modelers. Non-stationary data poses challenges, as traditional models often assume stationarity for accurate predictions. Analysts must address unit roots by employing transformations, such as differencing, to induce stationarity and facilitate model development.\n",
    "\n",
    "In predictive maintenance scenarios, unit roots play a crucial role in ensuring the accuracy of forecasting models. The long-term influence embedded in unit roots can significantly impact the reliability of predictions, making their identification and mitigation paramount for effective maintenance strategies.\n",
    "\n",
    "As we navigate this technical exploration, we will delve deeper into unit root testing methodologies, interpret the results, and explore strategies for handling non-stationary time series data. The theoretical underpinnings of unit roots provide a solid foundation for the practical applications that follow in our analytical journey.\n",
    "\n",
    "\n",
    "## How ‚Äúrandom‚Äù is your random?\n",
    "\n",
    "Let‚Äôs kick things off by generating a straightforward stationary series, but here‚Äôs a heads-up: not all ‚Äúrandom‚Äù is created equal. There are two main flavors of randomness ‚Äî true random and pseudorandom. Chances are, you‚Äôve been hanging out with pseudorandom more often because that‚Äôs the go-to for computers.\n",
    "\n",
    "In computing, generating truly random numbers is a challenge because computers are deterministic machines. Pseudorandom numbers, as the name suggests, are not genuinely random but instead are generated by algorithms that simulate randomness. These algorithms start with an initial value called a seed and use it to produce a sequence of numbers that appears random.\n",
    "\n",
    "## Seeds\n",
    "\n",
    "The seed is a crucial element in pseudorandom number generation. It serves as the starting point for the algorithm. If you use the same seed, you‚Äôll get the same sequence of pseudorandom numbers every time. This determinism can be advantageous in scenarios where you want reproducibility. For example, if you‚Äôre running a simulation or an experiment that involves randomness, setting the seed allows you to recreate the exact sequence of random numbers.\n",
    "\n",
    "On the flip side, changing the seed results in a different sequence of pseudorandom numbers. This property is often used to introduce variability in simulations or to provide different initial conditions for algorithms that use randomness.\n",
    "\n",
    "In summary, pseudorandom numbers are generated by algorithms, and the seed is the starting point for these algorithms. Controlling the seed allows you to control the sequence of pseudorandom numbers, providing a balance between determinism and variability in computer-generated randomness.\n",
    "\n",
    "Time to generate our pseudorandom distribution.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1992) # WOW this is our deterministic seed.\n",
    "\n",
    "def generate_stationary_series_pseudorandom(size=100):\n",
    "    stationary_series = np.random.randn(size)\n",
    "    return stationary_series\n",
    "```\n",
    "\n",
    "## Can we use true randomness?\n",
    "\n",
    "Now we might feel suprised that even randomness we are affecting most of the time is the deterministic random. But can we make true randomness, ensuring that no determism is behind it?\n",
    "\n",
    "Well, good news! We can tap into something truly physical ‚Äî atmospheric noise. Remember those flickering black and white dots on your TV screen? That‚Äôs our atmospheric noise, and we‚Äôre going to harness it to whip up some genuine randomness. So, your TV‚Äôs not just for shows; it‚Äôs your ticket out of the deterministic world.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def generate_stationary_series_random(size=100):\n",
    "    # Fetch truly random values from random.org atmospheric noise API\n",
    "    response = requests.get(f'https://www.random.org/integers/?num={size}&min=-10000&max=10000&col=1&base=10&format=plain&rnd=new')\n",
    "    if response.status_code == 200:\n",
    "        stationary_series = [int(value) for value in response.text.strip().split('\\n')]\n",
    "        return stationary_series\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch random values. Status code: {response.status_code}\")\n",
    "```\n",
    "\n",
    "\n",
    "Using this function we can genarate true randomness, Horray!\n",
    "\n",
    "Check the whole series in this link. Ensure you don‚Äôt miss out on new articles by following me.\n",
    "\n",
    "(from Understanding Predictive Maintenance ‚Äî Unit Roots and Stationarity)\n",
    "\n",
    "# ===============================================================================================\n",
    "\n",
    "\n",
    "## Unit Root\n",
    "\n",
    "We haven‚Äôt spoken about unit root yet, so we‚Äôll cover it now: A unit root (also called a unit root process or a difference stationary process) is a stochastic trend in a time series, sometimes called a ‚Äúrandom walk with drift‚Äù ‚Äî If a time series has a unit root, it shows a systematic pattern that is unpredictable [5].\n",
    "\n",
    "The reason why it‚Äôs called a unit root is because of the mathematics behind the process. At a basic level, a process can be written as a series of monomials (expressions with a single term). Each monomial corresponds to a root. If one of these roots is equal to 1, then that‚Äôs a unit root [5].\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Explained visually, if a series was purely stationary then any spikes in the curve would eventually return to it‚Äôs previous value ‚Äî in this figure above, the previous value is the x-axis itself. However, if a series has a unit root then it could settle at any point between the Pure Random Walk (blue) and the Purely Stationary (green). In this example it is the dotted line, however in reality we have no way of knowing where!\n",
    "\n",
    "In summary, the stationarity of a time series determines how easily it can be decomposed and forecasted using statistical techniques. Additionally, stationarity is determined by identifying if a series has a unit root using tests such as ADF and KPSS. Most of the time (all the time) a forex series will not be stationary which is one of the main reasons WHY it is so difficult to forecast. We will see a concreate example of this shortly.\n",
    "\n",
    "\n",
    "(from Explaining Stationarity and its Impact on Forecasting Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dd4df",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# =======================================================\n",
    "## 5Ô∏è‚É£ Making a Series Stationary\n",
    "\n",
    "To make the time series stationary, we can apply transformations to the data.\n",
    "\n",
    "---\n",
    "\n",
    "More likely than not your time series will not be stationary which means that you will have to identify the trends present in your series and manipulate the data to become stationary. After the trends are removed you can apply advanced modeling techniques while maintaining the valuable knowledge of the separated trends, which will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94f58a",
   "metadata": {},
   "source": [
    "## Differencing\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "```python\n",
    "# First-order difference\n",
    "diff_series = pd.Series(random_walk).diff().dropna()\n",
    "\n",
    "# Plot and test\n",
    "plot_series(diff_series, \"First Difference of Random Walk\")\n",
    "adf_test(diff_series, \"ADF Test - Differenced Series\")\n",
    "kpss_test(diff_series, \"KPSS Test - Differenced Series\")\n",
    "```\n",
    "\n",
    "## Difference Transform\n",
    "\n",
    "Differencing is a transform that helps stabilize the mean of the time series by removing changes in the level of a time series, which eliminates trend and seasonality. The first-order difference transform consists of taking the data point at the current time and subtracting it with the point before. The result is a dataset of differences between points at time t. If the first-order difference is stationary and random, then it is called a ‚Äúrandom walk‚Äù model.\n",
    "\n",
    "EQUATION\n",
    "\n",
    "IMAGE\n",
    "\n",
    "In this case, differencing does not produce the desired results. Even though the mean is stable, the variance just keeps increasing. In some cases, using the second-order difference transform would work but I decided to try out the logarithmic transform instead.\n",
    "\n",
    "(from Why Does Stationarity Matter in Time Series Analysis?)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "### Differencing Transform\n",
    "\n",
    "The most common transformation is to difference the time series. This is calculating the numerical change between each successive data point. Mathematically, this is written as:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Where d(t) is the difference at time t between the data points y(t) and y(t-1).\n",
    "\n",
    "We can plot the differenced data by using the diff() pandas method to simply calculate the differenced data as a column of our data-frame:\n",
    "\n",
    "```python\n",
    "# Take the difference and plot it\n",
    "data[\"Passenger_Diff\"] = data[\"#Passengers\"].diff()\n",
    "\n",
    "plotting(title='Airline Passengers', data=data, x='Month', y='Passenger_Diff',\n",
    "         x_label='Date', y_label='Passengers<br>Difference Transform')\n",
    "```\n",
    "\n",
    "Is the data now stationary? No.\n",
    "\n",
    "The mean is now constant and is oscillating about zero. However, we can clearly see the variance is still increasing through time.\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Differencing\n",
    "\n",
    "Differencing calculates the difference between two consecutive observations. It stabilizes the mean of a time series and thus reduces the trend [3].\n",
    "\n",
    "df[\"example_diff\"] = df[\"example\"].diff()\n",
    "\n",
    "\n",
    "IMAGE\n",
    "\n",
    "If you want to expand your knowledge on differencing, you should have a look at fractional differencing.\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## 3) Differencing\n",
    "\n",
    "Another method for removing trends in time series data is differencing. This is the process subtracting the value of one observation with the value of another observation x number of periods ago, where x is the time period lag. For instance, in the S&P 500 example, if the lag is one year then the differencing value on January 1, 2020 is equal to the actual price observed on January 1, 2020 minus the value observed on January 1, 2019. The Pandas library‚Äôs .diff(periods=x) method can be used to calculate an array of differentiating values. The period parameter denotes the lag used. My values are in daily increments which means a lag of 365 is equal to a year and a lag of 1 is equal to a day.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "The differencing also removed the upward trend of the time series although the variance is still time dependent.\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102c482",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "I organized my price data in a pandas dataframe. The index is set to the date column and the dates are sorted in ascending order. I obtained a year‚Äôs worth of OHLC prices ending on January 9, 2020.\n",
    "\n",
    "TABLE\n",
    "\n",
    "There are several transformations available in Python‚Äôs NumPy library including logarithms, square roots, and more. I created a new column for a few of these transformations by applying them to the adjusted closing price column.\n",
    "\n",
    "```python\n",
    "# Create transformation columns of the adjusted close price\n",
    "\n",
    "# Calculate the log of the adjusted close prices\n",
    "inx_df['adj_close_log'] = np.log(inx_df['adj_close'])\n",
    "\n",
    "# Calculate the square root of the adjusted close prices\n",
    "inx_df['adj_close_sqrt'] = np.sqrt(inx_df['adj_close'])\n",
    "\n",
    "# Calculate the cubed root of the adjusted close prices\n",
    "inx_df['adj_close_cbrt'] = np.cbrt(inx_df['adj_close'])\n",
    "```\n",
    "\n",
    "TABLE\n",
    "\n",
    "No single transformation method will universally turn all time series stationary, you will have to test them for yourself. The visualization of the logarithmic transformation is below.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "This particular transformation didn‚Äôt fully accomplish stationarity for this series. The range of the prices changed drastically and the upward trend of the series has been reduced which is a good first step. Logarithmic functions are inverses of exponential functions with the same base.\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d508b7b",
   "metadata": {},
   "source": [
    "\n",
    "## Logarithmic Transform\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Sometimes, differencing is not enough to remove trends in all non-stationary data. The logarithmic transform takes the log of each point and changes the data into a logarithmic scale. It is important to remember that the logarithmic transform must always be followed by the difference transform.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "As you can see above, the mean and variance level out and become constant. There are no signs of trends or strong seasonality.\n",
    "\n",
    "(from Why Does Stationarity Matter in Time Series Analysis?)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "To stabilise the variance, we apply the natural logarithm transform to the original data:\n",
    "\n",
    "```python\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Take the log and plot it\n",
    "data[\"Passenger_Log\"] = np.log(data[\"#Passengers\"])\n",
    "\n",
    "plotting(title='Airline Passengers', data=data, x='Month',\n",
    "         y='Passenger_Log', x_label='Date', y_label='Passenger<br>Log Transform')\n",
    "```\n",
    "\n",
    "The fluctuations are now on a consistent scale, but there is still a trend. Therefore, we now again have to apply the difference transform.\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Log transformation\n",
    "\n",
    "Log transformation stabilizes the variance of a time series [8].\n",
    "\n",
    "```python\n",
    "df[\"example_diff\"] = np.log(df[\"example\"].value)\n",
    "```\n",
    "\n",
    "IMAGE\n",
    "\n",
    "As you can see, both the detrending with model fitting as well as the log transform alone did not make our example time series stationary. You can also combine different techniques to make a time series stationary:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156f605",
   "metadata": {},
   "source": [
    "## Logarithmic and Difference Transform\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "### Log Transformation + Differencing (useful for trending/seasonal series)\n",
    "\n",
    "```python\n",
    "# Simulate exponential growth series\n",
    "t = np.arange(100)\n",
    "exp_series = np.exp(0.03 * t) + np.random.normal(scale=0.5, size=100)\n",
    "\n",
    "plot_series(exp_series, \"Exponential Trend Series\")\n",
    "\n",
    "# Log + diff\n",
    "log_series = np.log(exp_series)\n",
    "log_diff = pd.Series(log_series).diff().dropna()\n",
    "\n",
    "plot_series(log_diff, \"Log-Differenced Series\")\n",
    "adf_test(log_diff, \"ADF Test - Log-Differenced Series\")\n",
    "```\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "### Logarithmic and Difference Transform\n",
    "\n",
    "Applying both logarithmic and difference transforms:\n",
    "\n",
    "```python\n",
    "# Take the difference and log and plot it\n",
    "data[\"Passenger_Diff_Log\"] = data[\"Passenger_Log\"].diff()\n",
    "\n",
    "plotting(title='Airline Passengers', data=data, x='Month', y='Passenger_Diff_Log',\n",
    "         x_label='Date', y_label='Passenger<br>Log and Difference')\n",
    "```\n",
    "\n",
    "Is the data now stationary? Yes!\n",
    "\n",
    "As we can see, the mean and variance is now constant and has no long term trend.\n",
    "\n",
    "(from Stationarity For Time Series)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0ae98",
   "metadata": {},
   "source": [
    "## Others ?\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## Detrending by (linear) model fitting\n",
    "\n",
    "Another way to remove the trend from a non-stationary time series is to fit a simple model (e.g., linear regression) to the data and then to model the residuals from that fit.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit model (e.g., linear model)\n",
    "X = [i for i in range(0, len(airpass_df))]\n",
    "X = numpy.reshape(X, (len(X), 1))\n",
    "y = df[\"example\"].values\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculate trend\n",
    "trend = model.predict(X)\n",
    "\n",
    "# Detrend\n",
    "df[\"example_detrend\"] = df[\"example\"].values - trend\n",
    "```\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "## 2) Rolling Means\n",
    "\n",
    "You can subtract the rolling mean from a time series. This works especially well when the mean is time dependent. A rolling mean is the mean of the previous x number of observations in the series, where the time between each observation is consistent. You have to decide which time window works best for your data. Because I am using daily trading data I selected a window of 20 because that is how many trading days there are in a month, although this is not a universal window for financial data.\n",
    "\n",
    "Pandas‚Äô .rolling() method can be used to calculate this rolling mean. For instance, the code to calculate a 20 day rolling mean for my data is:\n",
    "\n",
    "```python\n",
    "inx_df[‚Äòadj_close‚Äô].rolling(window = 20).mean()\n",
    "```\n",
    "\n",
    "IMAGE\n",
    "\n",
    "I created a new array of the rolling mean subtracted from the original closing price column and charted it below to see if this improved stationarity in the series.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "This series appears to be much closer to stationarity. The upward trend is virtually gone but the variance is still apparent. For financial data, it is perfectly reasonable to remove the weighted rolling mean from the original data as well. The weighted rolling mean assigns a greater weight to more recent observations. In Python this is calculated with the .ewm() method, for my data the code is as follows:\n",
    "\n",
    "```python\n",
    "inx_df[‚Äòadj_close‚Äô].ewm().mean()\n",
    "```\n",
    "\n",
    "There are several parameters available in this method which determine the individual weights of the observations including com, span, and halflife.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03ac6f",
   "metadata": {},
   "source": [
    "## What to do when tests differ?\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Because there are several stationarity types, we can combine the ADF and KPSS tests to determine what transformations to make [7]:\n",
    "\n",
    "- If the ADF test result is stationary and the KPSS test result is non-stationary, the time series is difference stationary ‚Äî Apply differencing to time series and check for stationarity again [7].\n",
    "- If the ADF test result is non-stationary and the KPSS test result is stationary, the time series is trend stationary ‚Äî Detrend time series and check for stationarity again [7].\n",
    "\n",
    "(from Stationarity in Time Series ‚Äî A Comprehensive Guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e2e31",
   "metadata": {},
   "source": [
    "\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# =======================================================\n",
    "\n",
    "## Hands on Testing and Making Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0ca12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Hand`s on experience\n",
    "\n",
    "Now it is the time to make hands dirty by code. We will run some experiments to help you get fammilarized with article concepts. I reccomend you to reproduce it. Before we will dive deeply into stationarity I want to ask you question.\n",
    "\n",
    "And now we will add couple of examples how we can make this data not stationary, we are going to break our key rules of stationarity. After explanation we will plot all of them.\n",
    "\n",
    "## Linear Trend (Non-Constant Mean)\n",
    "\n",
    "```python\n",
    "def generate_non_stationary_linear_trend(size=100):\n",
    "    time = np.arange(size)\n",
    "    linear_trend = 0.5 * time\n",
    "    non_stationary_series = np.random.randn(size) + linear_trend\n",
    "    return non_stationary_series\n",
    "```\n",
    "\n",
    "Introducing a linear trend to violate the constant mean rule means adding a systematic increase or decrease over time. In the case of the non-stationary linear trend series, the values linearly increase over time. This violates the constant mean rule because the average value of the series is changing, indicating a shift in the underlying behavior of the process. Unit roots, in this context, contribute to the persistence of the linear trend, causing the variable‚Äôs value at any given time to be influenced by its past values.\n",
    "\n",
    "## Sine Amplitude (Non-Constant Variance)\n",
    "\n",
    "```python\n",
    "def generate_non_stationary_sin_amplitude(size=100):\n",
    "    time = np.arange(size)\n",
    "    amplitude = 0.5 + 0.02 * time\n",
    "    sin_amplitude_component = amplitude * np.sin(2 * np.pi * time / 10)\n",
    "    non_stationary_series = np.random.randn(size) + sin_amplitude_component\n",
    "    return non_stationary_series\n",
    "```\n",
    "\n",
    "Adding a sinusoidal component with increasing amplitude violates the constant variance rule. In the non-stationary seasonal component series, the amplitude of the sinusoidal component grows linearly with time. This results in fluctuations in the spread of data points, making the variance non-constant. Unit roots contribute to the persistence of the seasonal component, influencing the variance to vary as the amplitude changes.\n",
    "\n",
    "## Exponential Growth (Non-Constant Autocorrelation)\n",
    "\n",
    "Create results plot.\n",
    "\n",
    "```python\n",
    "def generate_non_stationary_exponential_growth(size=100, growth_rate=0.05):\n",
    "    time = np.arange(size)\n",
    "    exponential_growth_component = np.exp(growth_rate * time)\n",
    "    non_stationary_series = np.random.randn(size) + exponential_growth_component\n",
    "    return non_stationary_series\n",
    "```\n",
    "\n",
    "Incorporating an exponential growth pattern violates the constant autocorrelation rule. The non-stationary expanding amplitude series exhibits exponential growth, causing the autocorrelation pattern to change with increasing values. Unit roots play a role in introducing persistence into the time series, leading to challenges in modeling and forecasting. The presence of unit roots implies non-stationarity, indicating that the variable does not revert to a constant mean over time.\n",
    "\n",
    "## Start the experimets\n",
    "\n",
    "Execute the code and generate the timeseries and plot the results.\n",
    "\n",
    "```python\n",
    "# Example usage\n",
    "stationary_series_pseudorandom = generate_stationary_series_pseudorandom()\n",
    "non_stationary_linear_trend_series = generate_non_stationary_linear_trend()\n",
    "non_stationary_sin_amplitude_series = generate_non_stationary_sin_amplitude()\n",
    "non_stationary_exponential_growth_series = generate_non_stationary_exponential_growth()\n",
    "\n",
    "# Visualize the examples\n",
    "plot_multiple_series(stationary_series_pseudorandom, \n",
    "                     non_stationary_linear_trend_series, \n",
    "                     non_stationary_sin_amplitude_series, \n",
    "                     non_stationary_exponential_growth_series,\n",
    "                     titles=[\n",
    "                         'Stationary series',\n",
    "                         'Linear Trend (Non-Constant Mean)',\n",
    "                         'Sinusoidal Amplitude (Non-Constant Variance)',\n",
    "                         'Exponential Growth (Non-Constant Autocorrelation)'\n",
    "                     ])\n",
    "```\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Spotting a linear trend or exponential growth during exploratory data analysis is relatively straightforward, as these patterns exhibit clear visual cues. However, distinguishing between stationary and non-stationary states becomes challenging when dealing with sinusoidal amplitude. Visually, it‚Äôs hard to differentiate whether the amplitude is stationary or non-stationary just by looking at the data.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "This case will show the power of statistical tests. We have powerfull tools in our hands.\n",
    "\n",
    "```python\n",
    "_, adf_p_value_stationary, _, _, _, _ = adfuller(stationary_series_pseudorandom)\n",
    "_, adf_p_value_linear_trend, _, _, _, _ = adfuller(generate_non_stationary_linear_trend())\n",
    "_, adf_p_value_sin_amplitude, _, _, _, _ = adfuller(generate_non_stationary_sin_amplitude())\n",
    "_, adf_p_value_exponential_growth, _, _, _, _ = adfuller(generate_non_stationary_exponential_growth())\n",
    "\n",
    "# Print the results\n",
    "print(f'PseudoRandom ADF P-value (Stationary Series): {adf_p_value_stationary}')\n",
    "print(f'PseudoRandom ADF P-value (Linear Trend): {adf_p_value_linear_trend}')\n",
    "print(f'PseudoRandom ADF P-value (Sinusoidal Amplitude): {adf_p_value_sin_amplitude}')\n",
    "print(f'PseudoRandom ADF P-value (Exponential Growth): {adf_p_value_exponential_growth}')\n",
    "```\n",
    "\n",
    "The ADF test provides a clear distinction between stationary and non-stationary time series. In the first case, we can confidently reject the null hypothesis, indicating that the time series is stationary. However, for the other cases, we must accept the null hypothesis, concluding that the data is non-stationary. Specifically, in the case of sinusoidal amplitude, even though the non-stationarity is visually evident, the ADF test confirms our observation by not allowing us to reject the null hypothesis.\n",
    "\n",
    "## Practice the transformation\n",
    "\n",
    "Now, let‚Äôs have some fun with transformations and attempt to convert our non-stationary time series into a stationary one ‚Äî like a bit of reverse engineering. In real-life scenarios, determining the exact transformation needed is often a trial-and-error process. I recommend conducting exploratory data analysis, plotting the time series, and making empirical attempts. If a transformation renders the series stationary, you not only achieve stationarity but also gain valuable insights into the characteristics of your data.\n",
    "\n",
    "```python\n",
    "def make_linear_trend_stationary(series):\n",
    "    # Subtract the linear trend to make the mean constant.\n",
    "    time = np.arange(len(series))\n",
    "    linear_trend = 0.5 * time # Somehow we have found this trend :)\n",
    "    stationary_series = series - linear_trend\n",
    "    return stationary_series\n",
    "\n",
    "def make_sin_amplitude_stationary(series):\n",
    "    # Apply differencing to stabilize and make the variance constant.\n",
    "    diff_series = np.diff(series)\n",
    "    return diff_series\n",
    "\n",
    "def make_exponential_growth_stationary(series, epsilon=1e-8):\n",
    "    # Add a small constant to avoid zero or negative values\n",
    "    series = np.where(series <= 0, epsilon, series)\n",
    "    \n",
    "    # Add a small constant to avoid non-finite values\n",
    "    series += epsilon\n",
    "\n",
    "    # Apply the log for stabilization\n",
    "    series = np.log(series)\n",
    "    \n",
    "    # Take the first difference to remove the exponential growth\n",
    "    stationary_series = np.diff(series)\n",
    "    \n",
    "    return stationary_series\n",
    "```\n",
    "\n",
    "Having defined our transformation functions, it‚Äôs time to put them to work. Let‚Äôs apply these transformations to our non-stationary time series and see if we can successfully induce stationarity.\n",
    "\n",
    "```python\n",
    "# Apply transformations to make non-stationary examples stationary\n",
    "stationary_linear_trend = make_linear_trend_stationary(generate_non_stationary_linear_trend())\n",
    "stationary_sin_amplitude = make_sin_amplitude_stationary(generate_non_stationary_sin_amplitude())\n",
    "stationary_exponential_growth = make_exponential_growth_stationary(generate_non_stationary_exponential_growth())\n",
    "\n",
    "# Perform ADF test for the transformed series\n",
    "adf_p_value_stationary_linear_trend = adfuller(stationary_linear_trend)[1]\n",
    "adf_p_value_stationary_sin_amplitude = adfuller(stationary_sin_amplitude)[1]\n",
    "adf_p_value_stationary_exponential_growth = adfuller(stationary_exponential_growth)[1]\n",
    "\n",
    "# Print the results\n",
    "print(f'ADF P-value (Stationary Linear Trend): {adf_p_value_stationary_linear_trend}')\n",
    "print(f'ADF P-value (Stationary Sinusoidal Amplitude): {adf_p_value_stationary_sin_amplitude}')\n",
    "print(f'ADF P-value (Stationary Exponential Growth): {adf_p_value_stationary_exponential_growth}')\n",
    "```\n",
    "\n",
    "And how this data looks:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Great news! With our data now stationary, we confidently reject the null hypothesis in each case. Now, for a bit of fun, I‚Äôll take on the challenge of reverse engineering your random generation iteration with the given seed. Let‚Äôs see if I can unravel the mystery! üòÑ\n",
    "\n",
    "(from Understanding Predictive Maintenance ‚Äî Unit Roots and Stationarity)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "The P-value is just over 0.99, providing strong evidence that the dataset isn‚Äôt stationary. You‚Äôve learned the concept of differencing in the previous articles. Now you‚Äôll use it to calculate the N-th order difference. Here‚Äôs how the procedure looks for the first and second order:\n",
    "\n",
    "\n",
    "```python\n",
    "# First and second order difference\n",
    "df['Passengers_Diff1'] = df['Passengers'].diff()\n",
    "df['Passengers_Diff2'] = df['Passengers'].diff(2)\n",
    "\n",
    "# Don't forget to drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Plot\n",
    "plt.title('Airline Passengers dataset with First and Second order difference', size=20)\n",
    "plt.plot(df['Passengers'], label='Passengers')\n",
    "plt.plot(df['Passengers_Diff1'], label='First-order difference', color='orange')\n",
    "plt.plot(df['Passengers_Diff2'], label='Second-order difference', color='green')\n",
    "plt.legend();\n",
    "```\n",
    "\n",
    "And here‚Äôs the visualization:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "The differenced series looks more promising than the original data, but let‚Äôs use the ADF test to verify that claim:\n",
    "\n",
    "\n",
    "# Perform ADF test\n",
    "adf_diff_1 = adfuller(df['Passengers_Diff1'])\n",
    "adf_diff_2 = adfuller(df['Passengers_Diff2'])\n",
    "\n",
    "# Extract P-values\n",
    "p_1 = adf_diff_1[1]\n",
    "p_2 = adf_diff_2[1]\n",
    "\n",
    "# Print\n",
    "print(f'P-value for 1st order difference: {np.round(p_1, 5)}')\n",
    "print(f'P-value for 2nd order difference: {np.round(p_2, 5)}')\n",
    "\n",
    "Here‚Äôs the output:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "The first-order difference didn‚Äôt make the time series stationary, at least not at the usual significance level. Second-order differencing did the trick.\n",
    "\n",
    "You can see how manual testing of different differencing orders can be tedious. That‚Äôs why you‚Äôll write an automation function next.\n",
    "\n",
    "## Automating stationarity tests\n",
    "\n",
    "The automation function will accept the following parameters:\n",
    "\n",
    "- data: pd.Series ‚Äî time series values, without the datetime information\n",
    "- alpha: float = 0.05 ‚Äî significance level, set to 0.05 by default\n",
    "- max_diff_order: int = 10 ‚Äî the maximum time allowed to difference the time series\n",
    "\n",
    "Python dictionary is returned, containing differencing_order and time_series keys. The first one is self-explanatory, and the second one contains the differenced time series.\n",
    "\n",
    "The function will first check if the series is already stationary. If that‚Äôs the case, it‚Äôs returned as-is. If not, the ADF test is performed for every differencing order up to max_diff_order. The function keeps track of P-values and returns the one with the lowest differencing order that‚Äôs below the significance level alpha.\n",
    "\n",
    "Here‚Äôs the entire function:\n",
    "\n",
    "```python\n",
    "def make_stationary(data: pd.Series, alpha: float = 0.05, max_diff_order: int = 10) -> dict:\n",
    "    # Test to see if the time series is already stationary\n",
    "    if adfuller(data)[1] < alpha:\n",
    "        return {\n",
    "            'differencing_order': 0,\n",
    "            'time_series': np.array(data)\n",
    "        }\n",
    "    \n",
    "    # A list to store P-Values\n",
    "    p_values = []\n",
    "    \n",
    "    # Test for differencing orders from 1 to max_diff_order (included)\n",
    "    for i in range(1, max_diff_order + 1):\n",
    "        # Perform ADF test\n",
    "        result = adfuller(data.diff(i).dropna())\n",
    "        # Append P-value\n",
    "        p_values.append((i, result[1]))\n",
    "        \n",
    "    # Keep only those where P-value is lower than significance level\n",
    "    significant = [p for p in p_values if p[1] < alpha]\n",
    "    # Sort by the differencing order\n",
    "    significant = sorted(significant, key=lambda x: x[0])\n",
    "    \n",
    "    # Get the differencing order\n",
    "    diff_order = significant[0][0]\n",
    "    \n",
    "    # Make the time series stationary\n",
    "    stationary_series = data.diff(diff_order).dropna()\n",
    "    \n",
    "    return {\n",
    "        'differencing_order': diff_order,\n",
    "        'time_series': np.array(stationary_series)\n",
    "    }\n",
    "```\n",
    "\n",
    "Let‚Äôs now use it to make the airline passengers dataset stationary:\n",
    "\n",
    "```python\n",
    "ap_stationary = make_stationary(\n",
    "    data=df['Passengers']\n",
    ")\n",
    "\n",
    "plt.title(f\"Stationary Airline Passengers Dataset - Order = {ap_stationary['differencing_order']}\", size=20)\n",
    "plt.plot(ap_stationary['time_series']);\n",
    "```\n",
    "\n",
    "Here‚Äôs the visualization:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Just like before, second-order differencing is required to make the dataset stationary. But what if you decide for a different significance level? Well, take a look for yourself:\n",
    "\n",
    "```python\n",
    "ap_stationary = make_stationary(\n",
    "    data=df['Passengers'],\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "plt.title(f\"Stationary Airline Passengers Dataset - Order = {ap_stationary['differencing_order']}\", size=20)\n",
    "plt.plot(ap_stationary['time_series']);\n",
    "```\n",
    "\n",
    "Here‚Äôs the chart:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "You‚Äôll have to difference the dataset eight times for the significance level of 0.01. It would be a nightmare to revert, so you should probably stick with a higher significance level.\n",
    "\n",
    "(from Time Series From Scratch ‚Äî Stationarity Tests and Automation)\n",
    "\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a6ecb",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# ===============================================\n",
    "\n",
    "## 3Ô∏è‚É£ Visualizing Stationary vs Non-Stationary Series\n",
    "\n",
    "Let's generate a few examples to visualize what *stationary* and *non-stationary* behavior looks like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c1157",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Example 1: Stationary Series (White Noise)\n",
    "\n",
    "\n",
    "üí° This series has:\n",
    "\n",
    "* Constant mean (\\~0)\n",
    "* Constant variance\n",
    "* No trend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "stationary = np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(stationary)\n",
    "plt.title(\"Stationary Series: White Noise\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919dd5e",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Example 2: Non-Stationary Series (Trend)\n",
    "\n",
    "\n",
    "üí° You can see:\n",
    "\n",
    "* Increasing **mean** over time\n",
    "* Variance might still be constant, but the presence of a **trend** makes it non-stationary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = np.linspace(0, 10, 100) + np.random.normal(scale=1, size=100)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(trend)\n",
    "plt.title(\"Non-Stationary Series: Linear Trend\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32986b11",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Example 3: Non-Stationary Series (Changing Variance)\n",
    "\n",
    "üí° Characteristics:\n",
    "\n",
    "* Mean is constant\n",
    "* But variance is **increasing over time** ‚Äî makes it non-stationary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_change = np.random.normal(loc=0, scale=np.linspace(1, 5, 100), size=100)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(variance_change)\n",
    "plt.title(\"Non-Stationary Series: Changing Variance\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cfe74",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Example 4: Non-Stationary Series (Seasonality)\n",
    "\n",
    "üí° This series has:\n",
    "\n",
    "* Constant mean (on average)\n",
    "* Constant variance\n",
    "* But a **repeating pattern** ‚Üí still considered non-stationary in many cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a152b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(100)\n",
    "seasonal = 10 + np.sin(2 * np.pi * t / 12) + np.random.normal(scale=0.5, size=100)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(seasonal)\n",
    "plt.title(\"Non-Stationary Series: Seasonality\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91c8fa",
   "metadata": {},
   "source": [
    "\n",
    "## 4Ô∏è‚É£ How to Check Visually\n",
    "\n",
    "To visually check for stationarity:\n",
    "\n",
    "* Look for **trends**: rising or falling average ‚Üí non-stationary\n",
    "* Look for **variance shifts**: wider or tighter fluctuations over time ‚Üí non-stationary\n",
    "* Look for **seasonal patterns**: repeating cycles ‚Üí often treated as non-stationary\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Type              | Stationary? | Why                                                               |\n",
    "| ----------------- | ----------- | ----------------------------------------------------------------- |\n",
    "| White Noise       | ‚úÖ Yes       | Mean and variance constant, no pattern                            |\n",
    "| Trend             | ‚ùå No        | Mean increases over time                                          |\n",
    "| Changing Variance | ‚ùå No        | Variance increases over time                                      |\n",
    "| Seasonality       | ‚ùå No\\*      | Regular pattern, violates time-invariance (\\*may vary by context) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59ae67",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# ===============================================\n",
    "## 4Ô∏è‚É£ Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e1eba",
   "metadata": {},
   "source": [
    "Overall, understanding stationarity is vital to know how to approach the data. If the data is non-stationary, then certain transforms may help turn it into stationary data. The difference or logarithmic transforms are common techniques to make data stationarity. One method is not better than the other. The user needs to look at all methods and see each result before making a sound judgment. Using quantitative tools such as the ADF Test can give us a proper understanding of the properties of our data.\n",
    "\n",
    "(from Why Does Stationarity Matter in Time Series Analysis?)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this article we have described what a stationary time series is and how you can apply various transforms to make your data stationary. The log transform helps to stabilise the variance and the difference transfom stabilises the mean. We can then test for stationarity using the ADF test. The main importance of stationarity is that most forecasting models assume that the data holds that property. In my next article we will cover one of these forecasting models.\n",
    "\n",
    "The full code that generated the data, plots and ADF test in this post can be viewed here:\n",
    "\n",
    "LINK\n",
    "\n",
    "## References and Further Reading\n",
    "\n",
    "- Forecasting: Principles and Practice: https://otexts.com/fpp2/\n",
    "- ADF Test: https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/\n",
    "- Hypothesis Testing: https://towardsdatascience.com/z-test-simply-explained-80b346e0e239\n",
    "\n",
    "\n",
    "(from Stationarity For Time Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24eb98",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In time series forecasting, a time series, which has constant statistical properties (mean, variance, and covariance) and thus is independent of time, is described as stationary.\n",
    "\n",
    "Because of the constant statistical characteristics, a stationary time series is easier to model than a non-stationary time series. Thus, a lot of time series forecasting models assume stationarity.\n",
    "\n",
    "Stationarity can be checked either by visual assessment or by a statistical approach. The statistical approach checks for a unit root, an indicator of non-stationarity. The two most popular unit root tests are ADF and KPSS. Both are available in the Python stattools library [8,9].\n",
    "\n",
    "If a time series is non-stationary, you can try to make it stationary by differencing, log transforming, or removing the trend.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "All datasets are taken from the fma R package.\n",
    "\n",
    "[1] Hyndman RJ (2023). fma: Data sets from ‚ÄúForecasting: methods and applications‚Äù by Makridakis, Wheelwright & Hyndman (1998). R package version 2.5, http://pkg.robjhyndman.com/fma/.\n",
    "\n",
    "License: GPL-3 (https://cran.r-project.org/web/packages/fma/index.html)\n",
    "\n",
    "## References\n",
    "\n",
    "[2] Dickey, D. A. and Fuller, W. A. (1979). Distribution of the estimates for autoregressive time series with a unit root. J. Am. Stat. Assoc. 74, 427‚Äì431.\n",
    "\n",
    "[3] R. J. Hyndman, & G. Athanasopoulos (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3 . (Accessed on September 26, 2022).\n",
    "\n",
    "[4] Kwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?. Journal of econometrics, 54(1‚Äì3), 159‚Äì178.\n",
    "\n",
    "[5] D. C. Montgomery, C. L. Jennings, Murat Kulahci (2015) Introduction to Time Series Analysis and Forecasting, 2nd edition, John Wiley & Sons.\n",
    "\n",
    "[6] PennState (2023). S.3 Hypothesis Testing (Accessed on September 26, 2022).\n",
    "\n",
    "[7] statsmodels (2023). Stationarity and detrending (ADF/KPSS) (Accessed on March 10, 2023).\n",
    "\n",
    "[8] statsmodels (2023). statsmodels.tsa.stattools.adfuller (Accessed on September 26, 2022).\n",
    "\n",
    "[9] statsmodels (2023). statsmodels.tsa.stattools.kpss (Accessed on September 26, 2022).\n",
    "\n",
    "from (Stationarity in Time Series ‚Äî A Comprehensive Guide)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "And there you have it ‚Äî everything you should know about stationarity. The whole concept will get clearer in a couple of articles when you start with modeling and forecasting. For now, remember that a stationary process is easier to analyze and is required by most forecasting models.\n",
    "\n",
    "There‚Äôs still a couple of things left to cover before forecasting. These include train/test splits, metrics, and evaluations. All of these will be covered in the next article, so stay tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16ab9c",
   "metadata": {},
   "source": [
    "# =============================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts3_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
