{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8abd934",
   "metadata": {},
   "source": [
    "https://medium.com/data-science/non-stationarity-and-memory-in-financial-markets-fcef1fe76053\n",
    "\n",
    "# Stationarity and Memory in Financial Markets\n",
    "\n",
    "subtitle: Why you shouldn’t trust any stationarity test, and why memory has nothing to do with non-stationarity.\n",
    "\n",
    "Stationarity and time series predictability, a special case of which is time series memory, are notions that are fundamental to the quantitative investment process. However, these are often misunderstood by practitioners and researchers alike, as attests Chapter 5 of the recent book Advances in Financial Machine Learning. I’ve had the pleasure to elucidate these misconceptions with some attendees of The Rise Of Machine Learning in Asset Management at Yale last week after the conference, but I’ve come to think that the problem is so widespread that it deserves a public discussion.\n",
    "\n",
    "In this post I make a few poorly documented points about non-stationarity and memory in financial markets, some going against the econometrics orthodoxy. All arguments are backed by logic, maths, counter-examples and/or experiments with python code at the end.\n",
    "\n",
    "The arguments made here can be divided into practical and technical arguments:\n",
    "\n",
    "#### Technical Takeaways:\n",
    "\n",
    "- It is impossible to test whether a time series is non-stationarity with a single path observed over a bounded time interval — no matter how long. Every statistical test of stationarity makes an additional assumption about the family of diffusions the underlying process belongs to. Thus, a null hypothesis rejection can either represent empirical evidence that the diffusion assumption is incorrect, or that the diffusion assumption is correct but the null hypothesis (e.g. the presence of a unit root) is false. The statistical test by itself is inconclusive about which scenario holds.\n",
    "- Contrary to what is claimed in Advances in Financial Machine Learning, there is no “Stationarity vs. Memory Dilemma” (one has nothing to do with the other), and memory does not imply skewness or excess kurtosis.\n",
    "- Iterated differentiation of a time series à la Box-Jenkins does not make a time series more stationary, it makes a time series more memoryless; a time series can be both memoryless and non-stationary.\n",
    "- Crucially, non-stationarity but memoryless time series can easily trick (unit-root) stationarity tests.\n",
    "\n",
    "The notions of memory and predictability of time series are tightly related, and we discussed the latter in our Yellow Paper. I’ll take this opportunity to share our approach to quantifying memory in time series.\n",
    "\n",
    "#### Practical Takeaways:\n",
    "\n",
    "- That markets (financial time series specifically) are non-stationary makes intuitive sense, but any attempt to prove it statistically is doomed to be flawed.\n",
    "- Quantitative investment management needs stationarity, but not stationarity of financial time series, ‘stationarity’ or persistence of tradable patterns or alphas over (a long enough) time (horizon).\n",
    "\n",
    "\n",
    "## Stationarity\n",
    "\n",
    "Simply put, stationarity is the property of things that do not change over time.\n",
    "\n",
    "    Quant Investment Managers Need Stationarity\n",
    "\n",
    "At the core of every quantitative investment management endeavor is the assumption that there are patterns in markets that prevailed in the past, that will prevail in the future, and that one can use to make money in financial markets.\n",
    "\n",
    "A successful search for those patterns, often referred to as alphas, and the expectation that they will persist over time, is typically required prior to deploying capital. Thus stationarity is a wishful assumption inherent to quantitative investment management.\n",
    "\n",
    "    Stationarity In Financial Markets Is Self-Destructive\n",
    "\n",
    "However, alphas are often victim of their own success. The better an alpha, the more likely it will be copied by competitors over time, and therefore the more likely it is to fade over time. Hence, every predictive pattern is bound to be a temporary or transient regime. How long the regime will last depends on the rigor used in the alpha search, and the secrecy around its exploitation.\n",
    "\n",
    "The ephemerality of alphas is well documented; see for instance Igor Tulchinsky’s latest book, The Unrules: Man, Machines and the Quest to Master Markets, which I highly recommend.\n",
    "\n",
    "In regards to the widespread perception that financial markets are highly non-stationary though, non-stationarity is often meant in a mathematical sense and usually refers to financial time series.\n",
    "\n",
    "    Time Series Stationarity Can’t Be Disproved With One Finite Sample\n",
    "\n",
    "In the case of time series (a.k.a. stochastic processes), stationarity has a precise meaning (as expected); in fact two.\n",
    "\n",
    "A time series is said to be strongly stationary when all its properties are invariant by change of the origin of time, or time translation. A time series is said to be second-order stationary, or weakly stationary when its mean and auto-covariance functions are invariant by change of the origin of time, or time translation.\n",
    "\n",
    "Intuitively, a stationary time series is a time series whose local properties are preserved over time. It is therefore not surprising that it has been a pivotal assumption in econometrics over the past few decades, so much so that it is often thought that practitioners ought to first make a time series stationary before doing any modeling, at least in the Box-Jenkins school of thought.\n",
    "\n",
    "This is absurd for the simple reason that, (second order) stationarity, as a property, cannot be disproved from a single finite sample path. Yes, you read that right! Read on to understand why.\n",
    "\n",
    "But before delving into an almost philosophical argument, let’s take a concrete example.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Let’s consider the plot above. Is this the plot of a stationary time series? If you were to answer simply based on this plot, you would probably conclude that it is not. But I’m sure you see the trick coming, so you would probably want to run a so-called ‘stationarity test’, perhaps one of the most widely used, the Augmented-Dickey-Fuller test. Here’s what you’d get if you were to do so (source code at the end):\n",
    "\n",
    "ADF Statistic: 4.264155\n",
    "p-Value: 1.000000\n",
    "Critical Values:\n",
    "\t1%: -3.4370\n",
    "\t5%: -2.8645\n",
    "\t10%: -2.5683\n",
    "\n",
    "As you can see, the ADF test can’t reject the null hypothesis that the time series is an AR that has a unit root, which would (kind of) confirm your original intuition.\n",
    "\n",
    "Now, if I told you that the plot above is a draw from a Gaussian process with mean 100 and auto-covariance function\n",
    "\n",
    "EQUATION\n",
    "\n",
    "then I am sure you’d agree that it is indeed a draw from a (strongly) stationary time series. After all, both its mean and auto-covariance functions are invariant by time translation.\n",
    "\n",
    "If you’re still confused, here’s the same draw over a much longer time horizon:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "I’m sure you must be thinking that it looks more like what you’d expect from a stationary time series (e.g. it is visually mean-reverting). Let’s confirm that with our ADF test:\n",
    "\n",
    "ADF Statistic: -4.2702\n",
    "p-Value: 0.0005\n",
    "Critical Values:\n",
    "\t1%: -3.4440\n",
    "\t5%: -2.8676\n",
    "\t10%: -2.5700\n",
    "\n",
    "Indeed, we can reject the null hypothesis that the time series is non-stationarity at a 0.05% p-Value, which gives us strong confidence.\n",
    "\n",
    "However, the process hasn’t changed between the two experiments. In fact even the random path used is the same, and both experiments have enough points (at least a thousand each). So what’s wrong?\n",
    "\n",
    "Intuitively, although the first experiment had a large enough sample size, it didn’t span long enough a time interval to be characteristic of the underlying process, and there is no way we could have known that beforehand!\n",
    "\n",
    "The takeaway is that it is simply impossible to test whether a time series is stationary from a single path observed over a finite time interval, without making any additional assumption.\n",
    "\n",
    "Two assumptions are often made but routinely overlooked by practitioners and researchers alike, to an extent that results in misinformed conclusions; an implicit assumption and an explicit assumption.\n",
    "\n",
    "### 1. The Implicit Assumption\n",
    "\n",
    "Stationarity is a property of a stochastic process, not of a path. Attempting to test stationarity from a single path ought to implicitly rely on the assumption that the path at hand is sufficiently informative about the nature of the underlying process. As we saw above, this might not be the case and, more importantly, one has no way of ruling out this hypothesis. Because a path does not look mean-reverting does not mean that the underlying process is not stationary. You might not have observed enough data to characterize the whole process.\n",
    "\n",
    "Along this line, any financial time series, whether it passes the ADF test or not, can always be extended into a time series that passes the ADF test (hint: there exist stationary stochastic processes whose space of paths are universal). Because we do not know what the future holds, strictly speaking, saying that financial time series are non-stationary is slightly abusive, at least as much so as saying that financial time series are stationary.\n",
    "\n",
    "In the absence of evidence of stationarity, a time series should not be assumed to be non-stationary — we simply can’t favor one property over the other statistically. This works similarly to any logical reasoning about a binary proposition A: no evidence that A holds is never evidence that A does not hold.\n",
    "\n",
    "Assuming that financial markets are non-stationarity might make more practical sense as an axiom than assuming that markets are stationary for structural reasons. For instance, it wouldn’t be far fetch to expect productivity, global population, and global output, all of which are related to stock markets, to increase over time. However, would not make more statistical sense, and it is a working hypothesis that we simply cannot invalidate (in insolation) in light of data.\n",
    "\n",
    "### 2. The Explicit Assumption\n",
    "\n",
    "Every statistical test of stationarity relies on an assumption on the class of diffusions in which the underlying process’ diffusion must lie. Without this, we simply cannot construct the statistic to use for the test.\n",
    "\n",
    "Commonly used (unit root) tests typically assume that the true diffusion is an Autoregressive or AR process, and test the absence of a unit root as a proxy for stationarity.\n",
    "\n",
    "The implication is that such tests do not have as null hypothesis that the underlying process is non-stationary, but instead that the underlying process is a non-stationary AR process!\n",
    "\n",
    "Hence, empirical evidence leading to reject the null hypothesis could point to either the fact that the underlying process is not an AR, or that it is not stationary, or both! Unit root tests by themselves are not enough to rule out the possibility that the underlying process might not be an AR process.\n",
    "\n",
    "The same holds for other tests of stationarity that place different assumptions on the underlying diffusion. Without a model there is no statistical hypothesis test, and no statistical hypothesis test can validate the model assumption on which it is based.\n",
    "\n",
    "    Seek Stationary Alphas, Not Stationary Time Series\n",
    "\n",
    "Given that we cannot test whether a time series is stationary without making an assumption on its diffusion, we are faced with two options:\n",
    "\n",
    "- Make an assumption on the diffusion and test stationarity\n",
    "- Learn a predictive model, with or without assuming stationarity\n",
    "\n",
    "The former approach is the most commonly used in the econometrics literature because of the influence of the Box-Jenkins method, whereas the latter is more consistent with the machine learning spirit consisting of flexibly learning the data generating distribution from observations.\n",
    "\n",
    "Modeling financial markets is hard, very hard, as markets are complex, almost chaotic systems with very low signal-to-noise ratios. Any attempt to properly characterize market dynamics — for instance by attempting to construct stationary transformations — as a requirement for constructing alphas, is brave, counterintuitive, and inefficient.\n",
    "\n",
    "Alphas are functions of market features that can somewhat anticipate market moves in absolute or relative terms. To be trusted, an alpha should be expected to be preserved over time (i.e. be stationary in a loose sense). However, whether the underlying process itself is stationary or not (in the mathematical sense) is completely irrelevant. Value, size, momentum and carry are some examples of well documented trading ideas that worked for decades, and are unrelated to the stationarity of price or returns series.\n",
    "\n",
    "But enough with stationarity, let’s move on to the nature of memory in markets.\n",
    "\n",
    "## Memory\n",
    "\n",
    "Intuitively, a time series should be thought to have memory when its past values are related to its future values.\n",
    "\n",
    "To illustrate a common misunderstanding about memory, let’s consider a simple but representative example. In Advances in Financial Machine Learning, the author argues that\n",
    "\n",
    "“Most economic analyses follow one of two paradigms:\n",
    "\n",
    "- Box-Jenkins: returns are stationary, however memory-less\n",
    "- Engle-Ganger: Log-prices have memory, however they are non-stationary, and co-integration is the trick that make regression work on non-stationary time series […]”\n",
    "\n",
    "To get the best of both words, the author suggests constructing the weighted moving average process\n",
    "\n",
    "EQUATION\n",
    "\n",
    "whose coefficients are determined based on the notion of fractional differentiation with a fixed-window, as an alternative to log-returns (order 1 differentiation on log-prices). The author recommends choosing the smallest degree of fractional differentiation 0 < d < 1 for which the moving average time series passes the ADF stationarity test (at a given p-Value).\n",
    "\n",
    "The whole approach begs a few questions:\n",
    "\n",
    "- Is there really a dilemma between stationarity and memory?\n",
    "- How can we quantify memory in time series so as to confirm whether or not they are memoryless?\n",
    "- Assuming we could find a stationary moving average transformation with a lot of memory, how would that help us generate better alphas?\n",
    "\n",
    "    Quantifying Memory\n",
    "\n",
    "Intuitively, it is easy to see that moving average processes exhibit memory by construction (consecutive observations of a moving average are bound to be related as they are computed in part using the same observations of the input time series). However, not every time series that has memory is a moving average. To determine whether stationary time series have memory, one ought to have a framework for quantifying memory in any time series. We’ve tackled this problem in our Yellow Paper, and here’s a brief summary.\n",
    "\n",
    "The qualitative question guiding any approach to measuring memory in time series is the following. Does knowing the past inform us about the future? Said differently, does knowing all past values until now reduce our uncertainty about the next value of the time series?\n",
    "\n",
    "A canonical measure of uncertainty in a random variable is its entropy, when it exists.\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Similarly, the uncertainty left in a random variable after observing another random variable is typically measured by the conditional entropy.\n",
    "\n",
    "A candidate measure of the memory in a time series is therefore the uncertainty reduction about a future value of the time series that can be achieved by observing all past values, in the limit case of an infinite number of such past values. We call this the measure of auto-predictability of a time series.\n",
    "\n",
    "EQUATION\n",
    "\n",
    "When it exists, the measure of auto-predictability is always non-negative, and is zero if and only if all samples of the time series across time are mutually independent (i.e. the past is unrelated to the future, or the time series is memoryless).\n",
    "\n",
    "In the case of stationary time series, PR({X}) always exists and is given by the difference between the entropy of any observation and the entropy rate of the time series.\n",
    "\n",
    "EQUATION\n",
    "\n",
    "In our Yellow Paper, we propose a maximum-entropy based approach for estimating PR({X}). The following plot illustrates how much memory there is in stocks, futures and currencies.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "    Memory Has Nothing To Do With Stationarity\n",
    "\n",
    "A direct consequence of the discussion above is that a time series can both be stationary, and have a lot of memory. One does not preclude the other and, in fact, one is simply not related to the other.\n",
    "\n",
    "Indeed, in the case of stationary Gaussian processes, it can be shown that the measure of auto-predictability reads\n",
    "\n",
    "EQUATION\n",
    "\n",
    "It’s worth noting that PR({X})=0 if and only if the power spectrum is constant, that is, the time series is a stationary Gaussian white noise, otherwise PR({X})>0. A stationary white noise doesn’t lack memory because it is stationary, it lacks memory because it is, well […], a white noise!\n",
    "\n",
    "The more uneven the power spectrum is, the more memory there is in the time series. The flatter the auto-covariance function, the steeper the power spectrum, and therefore the higher the measure of auto-predictability, and the more memory the time series has. An example such flat auto-covariance function is the Squared-Exponential covariance function\n",
    "\n",
    "EQUATION\n",
    "\n",
    "in the limit where the input length scale parameter l goes to infinity.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "In short, there is no stationarity vs. memory dilemma. The confusion in practitioners’ minds comes from a misunderstanding of what goes on during iterated differentiation, as advocated by the Box-Jenkins methodology. More on that in the following section.\n",
    "\n",
    "    Memory Has Nothing To Do With Skewness/Kurtosis\n",
    "\n",
    "Another misconception about memory (see for instance Chapter 5, page 83 of the aforementioned book) is that there is “skewness and excess kurtosis that comes with memory”. This is also incorrect. As previously discussed it is possible to generate time series that are Gaussian (hence neither skewed nor leptokurtic), stationary, and have arbitrarily long memories.\n",
    "\n",
    "## Iterated Differentiation, Stationarity And Memory\n",
    "\n",
    "    Iterated Differentiation Does Not Make A Time Series More Stationary, It Makes A Time Series More Memoryless!\n",
    "\n",
    "Differentiation of (discrete-time) time series, in the Backshift Operator sense, works much like differentiation of curves learned in high-school.\n",
    "\n",
    "The more we keep differentiating a curve, the more likely the curve will undergo a discontinuity/abrupt change (unless of course it is infinitely differentiable).\n",
    "\n",
    "Intuitively, in the same vein, the more a time series is differentiated in the backshift operator sense, the more shocks (in a stochastic sense) the time series will undergo, and therefore the closer its samples will get to being mutually independent, but not necessarily identically distributed!\n",
    "\n",
    "Once a time series has been differentiated enough times that it has become memoryless (i.e. it has mutually independent samples), it is essentially a random walk, although not necessarily a stationary one. We can always construct a non-stationary time series that, no matter how many times it is differentiated, will never become stationary. Here’s an example:\n",
    "\n",
    "EQUATION\n",
    "\n",
    "Its order-1 differentiation is completely memoryless as increments of the Wiener process are independent.\n",
    "\n",
    "EQUATIONS\n",
    "\n",
    "Its variance function g(t) is time-varying, and therefore {y} is non-stationary.\n",
    "\n",
    "Similarly, the order-(d+1) differentiation of {y} is both memoryless and non-stationary for every d>0. Specifically, subsequent iterated differentiations read\n",
    "\n",
    "EQUATION\n",
    "\n",
    "and their time-dependent variance functions read\n",
    "\n",
    "EQUATION\n",
    "\n",
    "This expression clearly explodes in t for every d, and does not converge in d for any t. In other words, consecutive differentiations do not even-out the variance function, and therefore do not make this time series more stationary!\n",
    "\n",
    "    A Random Walk, Stationary Or Not, Would Typically Pass Most Unit-Root Tests!\n",
    "\n",
    "The confusion in practitioners’ minds about iterated differentiation and stationarity stems from the fact that most unit root tests will conclude that a memoryless time series is stationary, although this is not necessarily the case.\n",
    "\n",
    "Let’s consider the ADF test for instance.\n",
    "\n",
    "EQUATION\n",
    "\n",
    "If a time series {y} is memoryless but not stationary, the Ordinary Least Square (OLS) fit underpinning the ADF test cannot result in a perfect fit. How would this departure be accounted for by OLS with a large enough sample? As the time series is memoryless, OLS will typically find evidence that γ is close to 1, so that the ADF test ought to reject the null hypothesis that γ=0, to conclude that the time series does not have a unit root (i.e. is a stationary AR). The time-varying variance of {y} will typically be observed by the stationary noise term {e}.\n",
    "\n",
    "To illustrate this point, we generate 1000 random draws uniformly at random between 0 and 1, and we use these draws as standard deviations of 1000 independently generated mean-zero Gaussians. The result is plotted below.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "An ADF test run on this sample clearly reject the null hypothesis that the time series is a draw from an AR with unit root, as can be seen from the statistic below.\n",
    "\n",
    "ADF Statistic: -34.0381\n",
    "p-Value: 0.0000\n",
    "Critical Values:\n",
    "\t1%: -3.4369\n",
    "\t5%: -2.8644\n",
    "\t10%: -2.5683\n",
    "\n",
    "At this point, practitioners often jump to the conclusion that the time series ought to be stationary, which is incorrect.\n",
    "\n",
    "As previously discussed, a time series that is not a non-stationary AR is not necessarily stationary; it is either not an AR time series at all, or it is an AR that is stationary. In general the ADF test itself is inconclusive about which of the two assertions holds. In this example however, we know that the assumption that is incorrect is not non-stationarity, it is the AR assumption.\n",
    "\n",
    "## Concluding Thoughts\n",
    "\n",
    "Much attention has been devoted to the impact AI can have on the investment management industry in the media, with articles riding the AI hype, warning about the risk of backtest overfitting, making the case that the signal-to-noise ratio in financial markets rules out an AI revolution, or even arguing that AI has been around in the industry for decades.\n",
    "\n",
    "In these media coverages machine learning is often considered to be a static field, exogenous to the finance community, a set of general methods developed by others. However, the specificities of the asset management industry warrant the emergence of new machine learning methodologies, crafted with a finance-first mindset from the ground up, and questioning long-held dogmas. One of the biggest hurdles to the emergence of such techniques is perhaps the widespread misunderstanding of simple but fundamental notions, such as stationarity and memory, that are at the core of the research process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b634110",
   "metadata": {},
   "source": [
    "# ====================================================================\n",
    "# ====================================================================\n",
    "# ====================================================================\n",
    "# ====================================================================\n",
    "# ====================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73fc8a",
   "metadata": {},
   "source": [
    "https://medium.com/data-science/when-a-time-series-only-quacks-like-a-duck-10de9e165e\n",
    "\n",
    "# When A Time Series Only Quacks Like A Duck\n",
    "\n",
    "subtitle: Testing for Stationarity Before Running Forecast Models. With Python. And A Duckling Picture.\n",
    "\n",
    "ADF, KPSS, OSCB, and CH tests for stationarity and for a stable seasonal pattern; and how to deal with them if they provide contradictory results.\n",
    "\n",
    "To avoid a trap that could lead to a deficient forecast model, we will apply the ADF and the KPSS tests in parallel to check if the time series not only quacks like a duck, but also waddles like waterfowl is supposed to. We will also run the OCSB and CH tests to check if seasonal differencing is required.\n",
    "\n",
    "Our source consists of 1200 months of historical temperature records for the small (and entirely fictional) town of Lower Tidmarsh, East Dakotahoma. The Lower Tidmarsh town archive was destroyed by a kitchen fire in the 1980s before (or, as some residents told us, because) the volunteer fire brigade came to the rescue. The temperature records had to be reconstructed by interviewing the two centennial residents. The time series is synthetic, consisting of a sinusoidal seasonal component that mirrors the harsh winters and moderate summers in East Dakota; a global warming trend over the past century; and a white noise component representing the estimation uncertainty.\n",
    "\n",
    "You can download the small Temp.csv source file (~33 kB) from Google Drive via the link shown above. The Jupyter notebook is available via the second link.\n",
    "\n",
    "## 0. Dependencies\n",
    "\n",
    "```python\n",
    "# Stationarity\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pmdarima as pmd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "\n",
    "ALPHA = 0.05                        # significance level\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2671e2e9",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "```python\n",
    "# read the source data file\n",
    "df = pd.read_csv(\"Temp.csv\")\n",
    "df\n",
    "```\n",
    "\n",
    "Download the source data file Temp.csv.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Pandas imports .csv date columns as objects/strings. Therefore, we convert the dates to datetime, set an index, and derive year and month from the index.\n",
    "\n",
    "\n",
    "```python\n",
    "# convert objects/strings to datetime and numbers; set datetime index\n",
    "df = df.dropna()\n",
    "df.columns = [\"idx\", \"Date\", \"Temp\"]             # rename columns\n",
    "df.Date = df[\"Date\"]            \n",
    "df.Date = pd.to_datetime(df.Date)       # convert imported object/string to datetime\n",
    "df.set_index(df.Date, inplace=True)     # set Date as index\n",
    "df[\"year\"] = df.index.year\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "df[\"idx\"] = np.int64(df[\"idx\"])\n",
    "\n",
    "df.info()\n",
    "```\n",
    "\n",
    "Let’s create a pandas pivot table to look at the source data in tabular form.\n",
    "\n",
    "```python\n",
    "# tabular view of our source data via pivot table\n",
    "pivot = pd.pivot_table(\n",
    "    df, values='Temp', index='month', columns='year', \n",
    "    aggfunc='mean', margins=True, margins_name=\"Avg\", fill_value=\"\")\n",
    "pivot.transpose()\n",
    "```\n",
    "\n",
    "We use the pivot table to compute the 10-year rolling average temperature that will iron out the short-term fluctuations of seasonal peaks and dips, and then create a chart to study the long-term trend, if there is any.\n",
    "\n",
    "The plot shows a rising trend — a first indication that our time series is not stationary.\n",
    "\n",
    "```python\n",
    "# use the pivot table aggregations to plot the 10-year rolling average temperature and see the trend\n",
    "\n",
    "year_avg = pd.pivot_table(df, values='Temp', index='year', aggfunc='mean')\n",
    "year_avg['10 Years RA'] = year_avg['Temp'].rolling(10).mean()\n",
    "\n",
    "year_avg[['Temp','10 Years RA']].plot(figsize=(20,6))\n",
    "plt.title('Average Temperature')\n",
    "min_Y = df['year'].min()\n",
    "max_Y = df['year'].max()\n",
    "plt.xticks([x for x in range(max_Y, min_Y, -10)])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Before we can feed the temperature into a forecast model such as SARIMA, we need to test it for stationarity.\n",
    "\n",
    "We may be tempted to just kick off some kind of grid-search for suitable hyperparameters and then leave it to the auto-tuning process to identify the model with the lowest Akaike information criterion. But this can lead to the forecast quality trap mentioned earlier.\n",
    "\n",
    "- The information criteria represent the objective we want to minimize with respect to the autoregressive AR and moving average MA terms;\n",
    "- whereas the order of differencing must be determined in advance, by running tests for stationarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520c134",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Testing for Stationarity\n",
    "## 2.1 Stationarity and Differencing\n",
    "\n",
    "### Stationarity\n",
    "\n",
    "“A stationary time series is one whose properties don’t depend on the time at which the series is observed.” (Hyndman: 8.1 Stationarity and differencing | Forecasting: Principles and Practice (2nd ed) (otexts.com))\n",
    "\n",
    "A time series is stationary if its mean, variance, and autocorrelation structure do not change over time. If they are not time-invariant, the properties we use today to prepare a forecast would be different from the properties we would observe tomorrow. A process that is not stationary would elude our methods for using past observations to predict the future development. The time series itself does not need to remain a flat, constant line in past and future periods to be deemed stationary — but the patterns that determine its changes over time need to be stationary to make its future behavior predictable.\n",
    "\n",
    "The time series needs to exhibit:\n",
    "\n",
    "- time-invariant mean\n",
    "- time-invariant variance\n",
    "- time-invariant autocorrelations\n",
    "\n",
    "Time series with observations that are not stationary a priori can often be transformed to reach stationarity.\n",
    "\n",
    "### Inconstant Mean\n",
    "\n",
    "A series that shows a robust upward or downward trend does not have a constant mean. But if its data points tend to revert to the trendline after disturbances, the time series is trend-stationary. A transformation such as de-trending may turn it into a stationary time series that can be used in the forecast model. If the trend follows a predictable pattern, we can fit a trendline to the observations and then subtract it before we feed the de-trended series into the forecast model. Alternatively, we may be able to insert a datetime index into the model as an additional independent variable.\n",
    "\n",
    "If these de-trending measures do not suffice to realize a constant mean, we can investigate if the differences from one observation to the next have a constant mean.\n",
    "\n",
    "By differencing the time series — taking the difference between an observation y(t) and an earlier observation y(t-n) — we could obtain a stationary (mean-reverting) series of the changes.\n",
    "\n",
    "A time series in which any observation only depends on one or more of its predecessors (separated by a few lags), plus or minus some random error, is called a random walk. The differences between observations have a zero mean, apart from the error term, which by definition has a zero mean itself if it does not contain a signal with valuable information for the forecast. Random walks may exhibit long phases of apparent trends, up or down, followed by unpredictable changes of direction. A constant average trend requires one order of differencing.\n",
    "\n",
    "A time series in which the differences between neighboring observations have a non-zero mean will tend to drift upwards (positive mean) or downwards (negative mean). We difference a time series with drift to get a series with constant mean.\n",
    "\n",
    "Some time series require two rounds of differencing. The changes between observations are not constant (no constant “speed” between observations), but the change rate may be stable (constant “acceleration” or “deceleration”). If two rounds of differencing do not suffice to make a time series stationary, a third round is rarely justifiable. Rather, the properties of the time series should be investigated more closely.\n",
    "\n",
    "A time series with seasonality will exhibit patterns that repeat after a constant number of periods: temperatures in January differ from those in July, but January temperatures will be at a similar level between years. Seasonal differencing takes the difference between an observation and its predecessor that is S lags removed, with S being the number of periods in a full season, like 12 months in a year or 7 days in a week.\n",
    "\n",
    "If both the trend and the seasonal pattern are relatively time-invariant, the differenced time series (first-differenced with respect to the trend; and seasonally-differenced with respect to the seasonality) will have an approximately constant mean.\n",
    "\n",
    "### Inconstant variance\n",
    "\n",
    "If the time series takes on the shape of an expanding or narrowing funnel, then its observations fluctuate around its trend with an increasing or decreasing variance over time. Its variance is not time-invariant.\n",
    "\n",
    "By taking the logarithm of the observations, their square root, or by applying a Box-Cox-transformation, we may be able to stabilize the variance through transformations. After the forecast, we could reverse these transformations.\n",
    "\n",
    "### Inconstant autocorrelation structure\n",
    "\n",
    "The correlation and covariance between two observations y(t) and y(t-1), for any given t, do not remain constant over time. For stationarity, the autocorrelations should be time-invariant.\n",
    "\n",
    "### PSA #1: Determine Stationarity Before Fitting A Model\n",
    "\n",
    "The required order of differencing is a parameter that should be determined in advance, before fitting a forecast model to the data. A tuning algorithm can test any combinations of hyperparameters against a chosen benchmark such as the Akaike information criterion. But some of the hyperparameters may neutralize each other’s effects. A hyperparameter search in a SARIMA model will trade autoregressive AR and moving-average MA terms for changes in the order of differencing.\n",
    "\n",
    "“It is important to note that these information criteria tend not to be good guides to selecting the appropriate order of differencing (d) of a model, but only for selecting the values of p and q. This is because the differencing changes the data on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable. So we need to use some other approach to choose d, and then we can use the AICc to select p and q.” (Hyndman, 8.6 Estimation and order selection | Forecasting: Principles and Practice (2nd ed) (otexts.com)).\n",
    "\n",
    "Thus, if a hyperparameter search attempts to determine the order of differencing in parallel with the other parameters, we may obtain an inferior forecast model. The search would find an order of differencing that apparently minimizes AIC or BIC. But it may have missed a model that could lead to more accurate predictions despite its higher AIC. The search algorithm is unaware that its objective, the information criterion, cannot compare models with different orders of differencing.\n",
    "\n",
    "Either the tuning algorithm should apply hypothesis tests to determine the appropriate order of differencing before it starts a grid search for the other hyperparameters; or the data scientist pins down the order of differencing and then limits the grid search to the remaining parameters such as the AR and MA terms.\n",
    "\n",
    "### PSA #2: Conduct Parallel Tests for Stationarity\n",
    "\n",
    "To find out if differencing is required, we can run four tests to obtain objective results, which a visual inspection of charts may miss:\n",
    "\n",
    "- Augmented Dickey-Fuller ADF\n",
    "- Kwiatkowski-Phillips-Schmidt-Shin KPSS\n",
    "- Osborn-Chui-Smith-Birchenhall OCSB for seasonal differencing\n",
    "- Canova-Hansen CH for seasonal differencing\n",
    "\n",
    "I will skip some other unit root tests, such as Phillips-Peron.\n",
    "\n",
    "These tests may return contradictory results in quite a few cases. The following example will demonstrate that ADF and KPSS should be evaluated in parallel, not in isolation. Many of us — I included, when I prepared my first forecasts — are used to rely on the ADF test as our default for stationarity tests; others prefer the KPSS test. Few among us, I suppose, routinely apply and then compare both tests to decide on differencing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa534d37",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Augmented Dickey-Fuller Test (pmdarima) — Quacks Like A Duck?\n",
    "\n",
    "- Null hypothesis: the series contains a unit root: it is not stationary.\n",
    "- Alternative hypothesis: there is no unit root.\n",
    "- Low p-values are preferable. If the test returns a p-value below the chosen significance level (e.g. 0.05), we reject the null and conclude that the series does not contain a unit root.\n",
    "- If the ADF test does not find a unit root, but the KPSS test does, the series is difference-stationary: it still requires differencing.\n",
    "- The pmdarima tests, both ADF and KPSS, provide as outputs the p-value; and a Boolean value that is the answer to the the question: “Should we difference?”\n",
    "\n",
    "```python\n",
    "# pmdarima - ADF test - should we difference?\n",
    "# ADF null hypothesis: the series is not stationary \n",
    "def ADF_pmd(x):\n",
    "    adf_test = pmd.arima.stationarity.ADFTest(alpha=ALPHA)\n",
    "    res = adf_test.should_diff(x)\n",
    "    conclusion = \"non-stationary\" if res[0] > ALPHA else \"stationary\"\n",
    "    resdict = {\"should we difference? \":res[1], \"p-value \":res[0], \"conclusion\":conclusion}\n",
    "    return resdict\n",
    "\n",
    "# call the ADF test:\n",
    "resADF = ADF_pmd(df[\"Temp\"])\n",
    "\n",
    "# print test result dictionary:\n",
    "print(\"ADF test result for original data:\")\n",
    "[print(key, \":\", value) for key,value in resADF.items()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a6891",
   "metadata": {},
   "source": [
    "## 2.3 Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS) (pmdarima) — But It Does Not Walk Like A Duck?\n",
    "\n",
    "- Null hypothesis: the series is stationary around a deterministic trend (trend-stationary).\n",
    "- Note that the KPSS test swaps the null hypothesis and alternative hypothesis, compared to the ADF test.\n",
    "- Alternative hypothesis: the series has a unit root. It is non-stationary.\n",
    "- High p-values are preferable. If the test returns a p-value above the chosen significance level (e.g. 0.05), we conclude that it appears to be (at least trend-)stationary.\n",
    "- If the KPSS test does not find a unit root, but the ADF test does, the series is trend-stationary: it requires differencing (or other transformations such as de-trending) to remove the trend.\n",
    "\n",
    "```python\n",
    "# pmdarima - KPSS test -  should we difference?\n",
    "# null hypothesis: the series is at least trend stationary \n",
    "def KPSS_pmd(x):\n",
    "    kpss_test = pmd.arima.stationarity.KPSSTest(alpha=ALPHA)\n",
    "    res = kpss_test.should_diff(x)\n",
    "    conclusion = \"not stationary\" if res[0] <= ALPHA else \"stationary\"\n",
    "    resdict = {\"should we difference? \":res[1], \"p-value \":res[0], \"conclusion\":conclusion}\n",
    "    return resdict\n",
    "\n",
    "# call the KPSS test:\n",
    "resKPSS = KPSS_pmd(df[\"Temp\"])\n",
    "\n",
    "# print test result dictionary:\n",
    "print(\"KPSS test result for original data:\")\n",
    "[print(key, \":\", value) for key,value in resKPSS.items()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccc029",
   "metadata": {},
   "source": [
    "## 2.4 Compare the ADF and KPSS Test Results (pmdarima)\n",
    "\n",
    "```python\n",
    "# compare ADF and KPSS result\n",
    "test_values = zip(resADF.values(), resKPSS.values())\n",
    "dict_tests = dict(zip(resADF.keys(), test_values))\n",
    "df_tests = pd.DataFrame().from_dict(dict_tests).transpose()\n",
    "df_tests.columns = [\"ADF\", \"KPSS\"]\n",
    "print(\"Stationarity test results for original data:\")\n",
    "df_tests\n",
    "```\n",
    "\n",
    "Thus, the pmdarima tests return conflicting results.\n",
    "\n",
    "IMAGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c236e26",
   "metadata": {},
   "source": [
    "\n",
    "## 2.5 Order of Differencing Recommended by ADF and KPSS\n",
    "\n",
    "pmdarima also offers a method that returns the recommended order of first-differencing.\n",
    "\n",
    "The recommendations are contradictory as well, because the same ADF and KPSS tests are used to derive them.\n",
    "\n",
    "But we will come back to these orders of differencing later, when we will wrap up our findings and decide how to proceed.\n",
    "\n",
    "```python\n",
    "# pmdarima also offers methods that suggest the order of first differencing, based on either ADF or the KPSS test\n",
    "\n",
    "n_adf = pmd.arima.ndiffs(df[\"Temp\"], test=\"adf\")\n",
    "n_kpss = pmd.arima.ndiffs(df[\"Temp\"], test=\"kpss\")\n",
    "n_diffs = {\"ADF ndiff\":n_adf, \"KPSS ndiff\":n_kpss}\n",
    "print(\"recommended order of first differencing for original data:\")\n",
    "[print(key, \":\", value) for key,value in n_diffs.items()]\n",
    "```\n",
    "\n",
    "Let’s check with the statsmodels.stattools tests if this is just a quirk in the pmdarima algorithm (hint: it is not).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee25e6",
   "metadata": {},
   "source": [
    "\n",
    "## 2.6 Augmented Dickey-Fuller Test (stattools) — Quacks Like A Duck?\n",
    "\n",
    "- We use the adfuller test of statsmodels.stattools to obtain additional information compared to the pmdarima tests.\n",
    "- Null hypothesis: the series contains a unit root, it is not stationary.\n",
    "- Alternative hypothesis: there is no unit root.\n",
    "- Low p-values are preferable. If the test returns a p-value below the chosen significance level (e.g. 0.05), we reject the null and conclude that the series does not contain a unit root. It appears to be stationary.\n",
    "- If the ADF test does not find a unit root, but the KPSS test does, the series is difference-stationary: it requires differencing.\n",
    "\n",
    "```python\n",
    "# We apply the ADF and KPSS tests of statsmodels.stattools:\n",
    "\n",
    "\n",
    "# statsmodels - ADF test\n",
    "# null hypothesis: There is a unit root and the series is NOT stationary \n",
    "# Low p-values are preferable\n",
    "# get results as a dictionary\n",
    "def ADF_statt(x):\n",
    "     adf_test = adfuller(x, autolag=\"aic\")\n",
    "     t_stat, p_value, _, _, _, _  = adf_test\n",
    "     conclusion = \"non-stationary (unit root)\" if p_value > ALPHA else \"stationary\"\n",
    "     res_dict = {\"ADF statistic\":t_stat, \"p-value\":p_value, \"should we difference?\": (p_value > ALPHA), \"conclusion\": conclusion}\n",
    "     return res_dict\n",
    "\n",
    "\n",
    "# call the ADF test:\n",
    "resADF = ADF_statt(df[\"Temp\"])\n",
    "\n",
    "# print dictionary of test results:\n",
    "print(\"ADF test result for original data:\")\n",
    "# [print(key, \":\", f'{value:.3f}') for key,value in resADF.items()]\n",
    "[print(key, \":\", value) for key,value in resADF.items()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0344fc",
   "metadata": {},
   "source": [
    "## 2.7 Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS) (stattools) — Does Not Walk Like A Duck?\n",
    "\n",
    "- Null hypothesis: the series is stationary around a deterministic trend (trend-stationary).\n",
    "- Alternative hypothesis: the series has a unit root. It is non-stationary.\n",
    "- High p-values are preferable. If the test returns a p-value above the chosen significance level (e.g. 0.05), we conclude that it appears to be at least trend-stationary.\n",
    "- If the KPSS test does not find a unit root, but the ADF test does, the series is trend-stationary: it requires differencing or other transformations to remove the trend.\n",
    "\n",
    "```python\n",
    "# statsmodels - KPSS test\n",
    "# more detailed output than pmdarima\n",
    "# null hypothesis: There series is (at least trend-)stationary \n",
    "# High p-values are preferable\n",
    "# get results as a dictionary\n",
    "def KPSS_statt(x):\n",
    "     kpss_test = kpss(x)\n",
    "     t_stat, p_value, _, critical_values  = kpss_test\n",
    "     conclusion = \"stationary\" if p_value > ALPHA else \"not stationary\"\n",
    "     res_dict = {\"KPSS statistic\":t_stat, \"p-value\":p_value, \"should we difference?\": (p_value < ALPHA), \"conclusion\": conclusion}\n",
    "     return res_dict\n",
    "\n",
    "\n",
    "# call the KPSS test:\n",
    "resKPSS = KPSS_statt(df[\"Temp\"])\n",
    "\n",
    "# print dictionary of test results:\n",
    "# [print(key, \":\", f'{value:.3f}') for key,value in resKPSS.items()]\n",
    "print(\"KPSS test result for original data:\")\n",
    "[print(key, \":\", value) for key,value in resKPSS.items()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a8cce9",
   "metadata": {},
   "source": [
    "## 2.8 Compare the ADF and KPSS results — ADF quacks like a duck, but KPSS does not walk like waterfowl\n",
    "\n",
    "```python\n",
    "# compare ADF and KPSS result\n",
    "test_values = zip(resADF.values(), resKPSS.values())\n",
    "dict_tests = dict(zip(resADF.keys(), test_values))\n",
    "df_tests = pd.DataFrame().from_dict(dict_tests).transpose()\n",
    "df_tests.columns = [\"ADF\", \"KPSS\"]\n",
    "print(\"Stationarity Tests for original data, before differencing:\")\n",
    "df_tests\n",
    "```\n",
    "\n",
    "## 2.9 Difference or Don’t Difference?\n",
    "\n",
    "- So the ADF test does not find a unit root even though the chart above shows a clear upward trend.\n",
    "- The KPSS test reports that the series is not stationary.\n",
    "\n",
    "How do we deal with the conflict? Is the KPSS test always correct?\n",
    "\n",
    "## 2.10 Visual Plausibility Check: Decomposition\n",
    "\n",
    "```python\n",
    "# decomposition - let's decompose the time series, so we can clearly see its rising trend\n",
    "# which confirms that the series is not stationary\n",
    "\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "def plot_stationarity(y, lags):\n",
    "   \n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # decompose the time series into trend, seasonality and residuals\n",
    "    decomp = sm.tsa.seasonal_decompose(y)\n",
    "    # decomp.plot()\n",
    "    # plt.show()\n",
    "    trend = decomp.trend\n",
    "    seas = decomp.seasonal\n",
    "   \n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(18)\n",
    "    \n",
    "\n",
    "    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 3), (1, 0))\n",
    "    ax3 = plt.subplot2grid((3, 3), (1, 1))\n",
    "    #ax4 = plt.subplot2grid((3, 3), (1, 1))\n",
    "    ax5 = plt.subplot2grid((3, 3), (2, 0))\n",
    "    ax6 = plt.subplot2grid((3, 3), (2, 1))\n",
    "\n",
    "    y.plot(ax=ax1)\n",
    "    ax1.set_title(\"Rolling 12-Month Temperature\")\n",
    "    ax1.set_title(\"Temperature\")\n",
    "\n",
    "    trend.plot(ax=ax2)\n",
    "    ax2.set_title(\"Trend Component\")\n",
    "\n",
    "    seas.plot(ax=ax3)\n",
    "    ax3.set_title(\"Seasonal Component\")\n",
    "\n",
    "    # resid.plot(ax=ax4)\n",
    "    # ax4.set_title(\"Residual Component\")\n",
    "    \n",
    "    plot_acf(y, lags=lags, zero=False, ax=ax5);\n",
    "    plot_pacf(y, lags=lags, zero=False, ax=ax6);\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# get the plots for the time series before differencing\n",
    "plot_stationarity(df[\"Temp\"], 10)\n",
    "\n",
    "```\n",
    "\n",
    "- The trend chart does not show a constant mean, but rather an upward trend. The series cannot be stationary.\n",
    "- The autocorrelation plot shows high and persistent autocorrelations in its ACF and PACF charts, with seasonal oscillations. The series cannot be stationary if it exhibits stable seasonality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca4a0e",
   "metadata": {},
   "source": [
    "\n",
    "## 2.11 First-Difference: Reaching Stationarity\n",
    "\n",
    "We apply the differencing method .diff() to the original time series; and then check for stationarity with both ADF and KPSS.\n",
    "\n",
    "```python\n",
    "# ADF and KPSS tests after differencing:\n",
    "\n",
    "n_diff = max(n_adf, n_kpss)   \n",
    "df_diff1 = df[\"Temp\"].diff(n_diff).dropna()\n",
    "\n",
    "resADF = ADF_statt(df_diff1)\n",
    "resKPSS = KPSS_statt(df_diff1)\n",
    "test_values = zip(resADF.values(), resKPSS.values())\n",
    "dict_tests = dict(zip(resADF.keys(), test_values))\n",
    "df_tests = pd.DataFrame().from_dict(dict_tests).transpose()\n",
    "df_tests.columns = [\"ADF\", \"KPSS\"]\n",
    "\n",
    "print(\"Stationary after 1 round of first-differencing?\")\n",
    "df_tests\n",
    "```\n",
    "\n",
    "ADF and KPSS agree that the differenced series is stationary. The differenced series not only quacks like a duck, it also walks like one.\n",
    "\n",
    "```python\n",
    "# plot the differenced series\n",
    "plot_stationarity(df_diff1, 25)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfb242",
   "metadata": {},
   "source": [
    "\n",
    "## 2.12 Stationary — But What About The Seasonality?\n",
    "\n",
    "We have applied first-differences and received favorable test results from ADF and KPSS. Though the ACF plot still shows seasonal fluctuations.\n",
    "\n",
    "Let’s run the OCSB and CH tests to decide if we need a helping of seasonal differencing as well.\n",
    "\n",
    "The pmdarima implementations of both tests return the recommended orders of seasonal differencing.\n",
    "\n",
    "Osborn-Chui-Smith-Birchenhall OCSB Test:\n",
    "\n",
    "- Null hypothesis: the series contains a seasonal unit root\n",
    "- It uses a Dickey-Fuller type regression. (ocsb: OCSB test in seastests: Seasonality Tests (rdrr.io) )\n",
    "\n",
    "Canova-Hansen Test for Seasonal Stability:\n",
    "\n",
    "- Null hypothesis: the seasonal pattern is stable over time\n",
    "\n",
    "```python\n",
    "# time series before first differencing\n",
    "# OCSB test that returns the recommended order of seasonal differencing:\n",
    "n_ocsb = pmd.arima.OCSBTest(m=12).estimate_seasonal_differencing_term(df[\"Temp\"])\n",
    "\n",
    "\n",
    "# CH test that returns the recommended order of seasonal differencing:\n",
    "n_ch = pmd.arima.CHTest(m=12).estimate_seasonal_differencing_term(df[\"Temp\"])\n",
    "\n",
    "\n",
    "# seasonal differencing recommendation:\n",
    "print(\"time series before first differencing -\")\n",
    "n_seasdiffs = {\"recommended order of seasonal differencing\":\"\", \"OCSB recommendation\":n_ocsb, \"nCH recommendation\":n_ch}\n",
    "[print(key, \":\", value) for key,value in n_seasdiffs.items()]\n",
    "```\n",
    "\n",
    "2.12a When we investigate the original data, we observe another conflict, this time about seasonal differencing:\n",
    "\n",
    "- The OCSB does not identify a need for seasonal differencing, similar to the ACF for first differencing.\n",
    "- The CH test does recommend 1 order of seasonal differencing, similar to KPSS for first differencing.\n",
    "\n",
    "2.12b When we run OCSB and CH on the first-differenced data we have generated in chapter 2.11, then OCSB and CH agree that first-differencing has obviated the need for any seasonal differencing.\n",
    "\n",
    "```python\n",
    "# time series after first differencing\n",
    "# OCSB test that returns the recommended order of seasonal differencing:\n",
    "n_ocsb = pmd.arima.OCSBTest(m=12).estimate_seasonal_differencing_term(df_diff1)\n",
    "\n",
    "\n",
    "# CH test that returns the recommended order of seasonal differencing:\n",
    "n_ch = pmd.arima.CHTest(m=12).estimate_seasonal_differencing_term(df_diff1)\n",
    "\n",
    "\n",
    "# seasonal differencing recommendation:\n",
    "print(\"time series after first differencing -\")\n",
    "n_seasdiffs = {\"recommended order of seasonal differencing\":\"\", \"OCSB recommendation\":n_ocsb, \"nCH recommendation\":n_ch}\n",
    "[print(key, \":\", value) for key,value in n_seasdiffs.items()]\n",
    "```\n",
    "\n",
    "Conversely, if OCSB or CH had suggested to difference, we would have created a seasonally differenced series by appending the .diff(12) method to the original series.\n",
    "\n",
    "Syntax for differencing in pandas: If y is the variable that represents the series of undifferenced data, then:\n",
    "\n",
    "- y.diff(1) for first-differencing\n",
    "- y.diff(12) for seasonal differencing if the seasonality has a periodicity of 12 months. The recommended order of seasonal differencing would be multiplied by the periodicity of 12 to inform the pandas function .diff() about the number of lags it should use to jump from end of the seasonal period to the preceding end.\n",
    "- y.diff(1).diff(12) or y.diff(12).diff(1) — for combining both first- and seasonal differencing in a one-liner. The sequence of first- and seasonal differencing is not relevant — the results would be the same.\n",
    "- Rules for identifying ARIMA models (duke.edu):\n",
    "- “Rule 12: If the series has a strong and consistent seasonal pattern, then you must use an order of seasonal differencing (otherwise the model assumes that the seasonal pattern will fade away over time).\n",
    "- However, never use more than one order of seasonal differencing or\n",
    "- more than 2 orders of total differencing (seasonal+nonseasonal).”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc337dbf",
   "metadata": {},
   "source": [
    "\n",
    "## 2.13 ADF and KPSS Conflicts — How Do We Deal With Them?\n",
    "\n",
    "If the ADF and KPSS tests return conflicting results, how do we proceed: difference or don’t difference?\n",
    "\n",
    "As a general rule:\n",
    "\n",
    "- Neither the ADF test nor the KPSS test will confirm or disconfirm stationarity in isolation. Run both tests to decide if you should difference.\n",
    "- If a least one of the tests claims to have found non-stationarity, you should difference. An unambiguous confirmation of duckiness (stationarity) requires that both tests confirm the quacking and the waddling.\n",
    "\n",
    "A more specific explanation:\n",
    "\n",
    "There are 4 possible combinations of KPSS and ADF test results\n",
    "\n",
    "- If KPSS and ADF agree that the series is stationary (KPSS with high p-value, ADF with low p-value): Consider it stationary. No need to difference it.\n",
    "- ADF finds a unit root; but KPSS finds that the series is stationary around a deterministic trend (ADF and KPSS with high p-values). Then, the series is trend-stationary and it needs to be detrended. Difference it. Alternatively, a transformation may rid it of its trend.\n",
    "- ADF does not find a unit root; but KPSS claims that it is non-stationary (ADF and KPSS with low p-values). Then, the series is difference-stationary. Difference it.\n",
    "- If KPSS and ADF agree that the series is non-stationary (KPSS with low p-value; ADF with high p-value): Consider it non-stationary. Difference it.\n",
    "\n",
    "Let’s translate this heuristic to Python:\n",
    "\n",
    "For first-differencing, we take the higher of the orders which ADF and KPSS recommend.\n",
    "\n",
    "EQUATION CODE\n",
    "\n",
    "For seasonal differencing, we take the higher of the orders which OCSB and CH recommend. To avoid over-differencing, we should check if first-order differencing already arrives at stationarity.\n",
    "\n",
    "```python\n",
    "# seasonal differencing: combine the OCSB and CH test results\n",
    "n_sdiff = max(n_ocsb, n_ch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADF and KPSS tests after first differencing AND seasonal differencing:\n",
    "# use the larger recommended order of first and seasonal differencing, respectively\n",
    "\n",
    "if n_diff * n_sdiff != 0:                            # both first and seasonal differencing orders?\n",
    "    df_diff2 = df[\"Temp\"].diff(n_diff).diff(n_sdiff).dropna()\n",
    "elif n_diff + n_sdiff != 0:                         # either first or seasonal differencing recommended, but not both?\n",
    "    df_diff2 = df[\"Temp\"].diff(max(n_diff,n_sdiff)).dropna()\n",
    "else:                                               # neither first nor seasonal orders >0          \n",
    "    df_diff2 = df[\"Temp\"]\n",
    "\n",
    "\n",
    "\n",
    "resADF = ADF_statt(df_diff2)\n",
    "resKPSS = KPSS_statt(df_diff2)\n",
    "test_values = zip(resADF.values(), resKPSS.values())\n",
    "dict_tests = dict(zip(resADF.keys(), test_values))\n",
    "df_tests = pd.DataFrame().from_dict(dict_tests).transpose()\n",
    "df_tests.columns = [\"ADF\", \"KPSS\"]\n",
    "df_tests\n",
    "```\n",
    "\n",
    "```python\n",
    "# after first AND seasonal differencing: compare ADF, KPSS, OCSB and CH results\n",
    "n_adf = pmd.arima.ndiffs(df_diff2, test=\"adf\")\n",
    "n_kpss = pmd.arima.ndiffs(df_diff2, test=\"kpss\")\n",
    "n_ocsb = pmd.arima.OCSBTest(m=12).estimate_seasonal_differencing_term(df_diff2)\n",
    "n_ch = pmd.arima.OCSBTest(m=12).estimate_seasonal_differencing_term(df_diff2)\n",
    "\n",
    "print(\"after 1 round of differencing - do we need more?\")\n",
    "n_diffs = {\"recommended additional differencing\":\"\", \"ADF first\":n_adf, \"KPSS first\":n_kpss, \n",
    "    \"OCSB seasonal\":n_ocsb, \"CH seasonal\":n_ch}\n",
    "[print(key, \":\", value) for key,value in n_diffs.items()]\n",
    "```\n",
    "\n",
    "After one round of differencing, the code runs all four tests again — ADF, KPSS, OCSB, and CH — to confirm if additional differencing might be required. In our example, all four tests agree that the 1 order of first-differencing we have applied in chapter 2.11 was enough to arrive at a stationary time series — which we can now hand over to a forecast model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
